[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A Time Series Analysis of Energy in the United States",
    "section": "",
    "text": "A study by Elise Rust as part of the Georgetown MS in Data Science degree\nSpring 2023\n\nEnergy and power are at the center of all aspects of modern civilization. Human society’s food, transportation, communication, infrastructure, and more are connected to and dependent on “the grid.” The resilience of that grid, the sources of power for that grid, and the ways in which our energy consumption ebbs and flows are at the forefront of technological development, public policy, and economic analysis and this study will delve into it all through the lens of statistical time series analysis.\nCheck out the project below!\n\nAbout Me\nIntroduction\nData Sources\nData Visualization\nExploratory Data Analysis\nARMA/ARIMA/SARIMA Models\nARIMAX/SARIMAX/VAR Models\nSpectral Analysis\nFinancial Time Series Models\nDeep Learning for TS\nConclusions"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "dv.html",
    "href": "dv.html",
    "title": "Data Vizes in TS",
    "section": "",
    "text": "Elise Rust\nANLY 560\nJan 2023\n\n1. Stock Market Analysis\nTake three stocks of streaming companies: Netflix (NFLX), Disney (DIS), and Apple (AAPL).\n\n\n                NFLX      DIS     AAPL      Dates       date\n2012-10-01  8.007143 46.62054 20.12959 2012-10-01 2012-10-01\n2012-10-02  8.065714 46.23555 20.18820 2012-10-02 2012-10-02\n2012-10-03  8.940000 46.97869 20.49776 2012-10-03 2012-10-03\n2012-10-04  9.524286 47.12194 20.35580 2012-10-04 2012-10-04\n2012-10-05  9.508571 47.42636 19.92200 2012-10-05 2012-10-05\n2012-10-08 10.502857 46.85334 19.48180 2012-10-08 2012-10-08\n\n\n\n\n\n\n\nNow add a hover feature to the plot\n\n\n\n\n\n\nWhat does the graph say:\nShown here are the adjusted closing stock prices for three streaming services (Netflix, Disney, and Apple) between 2013 and 2022. Netflix was the original streaming giant, founded in 1997 as the first place a user could stream on-demand shows and movies starting in 2007. While they initially dominated the space, and saw a steady uptick in market value through 2019. Disney and Apple are both larger companies that span markets: Disney has a long history of movie production and amusement parks (Disney World and Disney Land) and entered the streaming space with Disney+ in November 2019. Apple historically produced technology (computers, tablets, phones, etc.) and entered the streaming space with AppleTV in 2007. The rise of these competitors - especially Disney+ who, upon launching, removed all Disney films and content from Netflix for their own platform - has caused Netflix to take a hit in their stock price. This is clearly shown in 2019 when NFLX market cap went from 375 to 265 per share. All three companies took a hit at the beginning of the pandemic in March 2020, but bounced back quickly as lockdown forced the global population inside where many turned to streaming services. Netflix stock peaked at 681 per share in November of 2021, but crashed to 186 per share as they’ve begun to deal with inflation and increased streaming competition.\n\nCrypto –> Coinbase Analysis using Plotly\n\n\n           COIN.Open COIN.High COIN.Low COIN.Close COIN.Volume COIN.Adjusted\n2021-09-15    244.00    249.47   242.05     247.07     2148300        247.07\n2021-09-16    247.38    247.46   239.61     243.21     2596800        243.21\n2021-09-17    243.13    245.63   238.75     245.19     2948000        245.19\n2021-09-20    234.45    237.36   231.15     236.53     4192600        236.53\n2021-09-21    237.01    240.26   234.26     238.46     2174700        238.46\n2021-09-22    239.91    243.20   237.76     241.85     1660200        241.85\n\n\n\n\nGGPlot Plot\n\n\n\n\n\n\n\nPlotly Plot\n\n\n\n\n\n\nWhat does the graph say:\nCryptocurrency, including Coinbase ($COIN), has suffered a devastating drop in confidence and value throughout 2022. Market value peaked for COIN in November of 2021, at around 350 per share, before seeing a precipitious drop to just 53.72 by May of 2022. Since then, it has hovered around this lower price per share, with very few indications of gaining value in the near future. 2022 has been widely deemed the beginning of “crypto winter” as a number of high profile companies, including Coinbase, Bitcoin, and terraUSD, saw their market value drop by up to 70%. Many people have attributed this to a pessimistic global macro outlook when the US Fed hiked interest rates and investors pivoted from risky assets like crypto to safer assets.\n\n\n\n2. Plot the climate data (climate.csv) using Plotly\nThe original climate dataframe looks like so:\n\n\n      STATION                      NAME       DATE DAPR MDPR PRCP SNOW SNWD\n1 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-06   NA   NA    0    0   NA\n2 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-07   NA   NA    0    0   NA\n3 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-08   NA   NA    0    0   NA\n4 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-09   NA   NA    0    0   NA\n5 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-10   NA   NA    0    0   NA\n6 US1DCDC0009 WASHINGTON 2.0 SSW, DC US 2021-03-11   NA   NA    0    0   NA\n  TMAX TMIN TOBS WESD\n1   NA   NA   NA   NA\n2   NA   NA   NA   NA\n3   NA   NA   NA   NA\n4   NA   NA   NA   NA\n5   NA   NA   NA   NA\n6   NA   NA   NA   NA\n\n\nData Cleaning and Transformation\nAfter data cleaning and transformation, we will look at Monthly Precipitation Totals across the different stations. The transformed dataframe looks like:\n\n\n# A tibble: 12 × 8\n   Month Dalecarlia_Reservoir National_Arboretum   DC1   DC2 DC2.6 DC3.1 DC5.1\n   <chr>                <dbl>              <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 01                    2.19               1.78  0.29  0     1.48  0.31  1.26\n 2 02                    3.04               4.28  3.32  0     2.55  3.06  4.04\n 3 03                    2.61               3.66  2.45  0.45  0.01  3.15  2.53\n 4 04                    3.67               3.19  1.19  1.76  2.22  2.97  3.66\n 5 05                    3.44               3.75  1.02  4.03  2.3   3     2.88\n 6 06                    3.04               4.37  4.02  4.72  4.14  1.88  8.03\n 7 07                    3.99               5.33  3.14  0     3.93  4.33  4.03\n 8 08                    9.06               6.76  8.27  0     6.52  7.81 13.4 \n 9 09                    7.67               3.36  2.13  0.12  3.95  4.05  9.24\n10 10                    0                  0     2.12  3.32  4.19  3.94  5.86\n11 11                    0                  0     0.73  0.93  0.37  1.13  1.62\n12 12                    0                  0     0.42  0.3   0.67  0.67  0.91\n\n\n\nPlot using Plotly\n\nDaily Precipitation Totals\n\n\n\n\n\n\n\nWhat the graph is saying:\nSeven (7) different monitoring stations across the DC-Maryland-Virginia area have collected precipitation data periodically throughout 2021. Six of the locations are in DC, while Dalecarlia Reservoir is in Bethesda Maryland. The precipitation totals have been plotted by location between January 2021 and January 2022. DC 5.1 NW is the location with the highest precipitation totals, seeing highs in September 2021 (2.12 inches in a day) and October 2021 (over 2 inches multiple days). DC 2 and 2.6 had the lowest precipitation totals, rarely cresting 1 inch of rain any day of the year. The graph does indicate that most of the wards of DC do receive similar rainfall totals, which intuitively makes sense given the small geographic area of the sample. It is notable to see that Ward 5 has higher rainfall, though, so that people living in the area can better prepare for incoming storms.\n\nMonthly Totals\n\n\n\n\n\n\n\nWhat the graph is saying:\nThe same data as above is being examined, but this time precipitation totals were grouped by month so see cumulative effects. Similar trends are shown - notably, that Ward 5 has higher rainfall than surrounding areas, expecially in August - October, so residents should be prepared for incoming storms. On the whole, each location averages about 3-4 inches of rain per month, with Aug-Oct being the highest rainfall months and Nov-Dec being some of the driest months."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Hi! My name is Elise Rust and I plan to finish my MS in Data Science from Georgetown in Spring 2023 after completing my undergrad in Economics and Data Science from Dartmouth College in 2021. Academically and professionally, my goal is to use data science to do good in the world - whether that’s addressing the climate crisis or identifying vulnerable communities for educational funding or investigating hate speech/fraudulent text on social media. Whether it’s using statistical analysis, machine learning, natural language processing, or spatial reasoning I find great joy in taking data science problems of all types through the entire data science lifecycle. I love being able to explore the data behind these problems in order to generate meaningful and defensible change - and delivering complex solutions in concise and accessible ways to move that needle is where my greatest strengths lie.\nOutside of Data Science I love being outside - skiing, biathlon, climbing, biking, backpacking, and roller blading, especially with friends, is the best part of life! Plus, I love a good book.\nCurrently reading: - Braiding Sweetgrass by Robin Kimmerer  - A Thousand Brains by Jeff Hawkins  - Let Me Tell You What I Mean by Joan Didion\n\nFeel free to reach out if you have questions about this project or wanna chat! \nEmail: er844@georgetown.edu"
  },
  {
    "objectID": "dataviz.html",
    "href": "dataviz.html",
    "title": "Data Viz",
    "section": "",
    "text": "Data Visualization is the first step to understanding the major trends and patterns in our data. Before modeling or analyzing the data sources collected on the Data Sources page, we visualize key questions around energy production, consumption, and cost in the United States using Plotly, ggplot, and Tableau.\n\nPlot 1 - Total Electricity Production Across Fuel Sources\nSource: EIA.gov\n\n\n\n\n\n\nThe Energy Information Administration provides data on total electricity production broken down by state and fuel source between 1990 and 2022. State totals are aggregated so show national trends of different fuel sources over time. The fuel sources are comprised of sustainable energy sources such as wind, solar, geothermal, as well as dirtier energy like coal and petroleum. In 1990, coal was by far the leading fuel source nationwide at 3.2 billion Megawatthours produced. Solar was the smallest fuel sector at only 700,000 Megawatthours produced. Throughout the next decade, most fuel sources trended upwards as the population grew and generated an increased demand for electricity. Coal production peaked in 2007 at 40 billion Megawatthours before beginning a steady decline down to its lowest in 2020 at 1 billion Megawatthours.\n2008 marked a shift in electricity production away from coal towards natural gas, which overtook coal as the most abundant fuel source in 2015. Other cleaner energy sources, such as wind and solar, started to see notable growth as early as 2010, likely in response to growing public awareness of climate change. \nPlot 2 - Energy Production by State\nSource: EIA.gov\n\n\n\n\n\n\nIn order to break down which states are the largest producers of electricity - and therefore which states should be the focus of energy security and resilience efforts - electricity production by state is visualized. Between 1990 - 2022, the electricity production of each state is visualized and stacked against each other, ranked from most to least. The largest producer is by far Texas at 480 million Megawatt Hours in 2022. Texas is the only state with its own power grid, making it even more vulnerable to future meltdowns. Given the failure of the Texas grid in 2022, this makes Texas a great state to prioritize for electric grid resilience projects.\nAfter Texas, Pennsylvania is #2 at only 246 million Megawatt Hours in 2022, with Florida and California close behind at 240 and 197 million Megawatt Hours, respectively.\n Plot 3 - Petroleum Exports vs. Imports\nSource: Table 3.1 Petroleum Overview\nThis plot was generated using Tableau.\n A key indicator of energy resilience is the ratio of imports and exports of different fuels. Plot 3 examines Petroleum imports and exports in the U.S. between 1945 to 2022, as well as total petroleum field production. The Council on Foreign Relations offers a complete and thorough timeline of the U.S.’ dependence on oil and how it affects foreign relations - which helps explain some of the trends seen in the chart above.\nPetroleum exports (green) were constant until about 1980, when a slow upward trend began and then turned into exponential growth in 2005. The 1980s uptick is due to the diversification of U.S. energy consumption when the Reagan administration deregulated crude oil prices in 1981. The rapid growth in 2005 parallels an uptick in total petroleum field production and the U.S.’ commitment towards energy independence.\nPetroleum imports (red) have consistently been higher than exports over the last 80 years. Exports rose steadily in the 60s as a result of a legislative import quota, and then spiked in 1973 when the the import quota ended. Imports dropped in 1980 when Carter signed the Energy Security Act into law - which incentivized use of alternative fuel sources. Imports then grew over the next 30 years before dropping in 2007 as more national petroleum was produced and the US loosened its reliance on foreign petroleum.\n Plot 4 - Cost of Energy Table 3.3 Consumer Price Estimates for Energy by Source, 1970-2010\n\n\n\n\n\n\n\nPlot 4 examines how the cost of different fuel sources has changed over the past 40 years. Wider adoption of cleaner energy is contingent upon its competitiveness with traditional fossil fuels, making this a critical avenue of analysis. Coal, petroleum, and natural gas have all generally trended upwards since 1970, with coal moving from 38 cents/million Btu in 1970 to $2.42/million Btu in 2010. Natural gas has grown from 59 cents/million Btu to $7.67/million Btu in 2010. Petroleum has grown from $1.71 to $20.32/million Btu in the 30 year span. These price changes constitute a 536%, 1183%, and 1088% increase, respectively.\nCleaner fuels like biomass have grown 167% ($1.29 to $3.45/million Btu) while nuclear power grew 245% ($0.18 to $0.62/million Btu).\nIn 2010, the cheapest fuel source was easily nuclear power, with coal and biomass next in line. Thus, this analysis will pay particular attention to nuclear power; exploring the potential benefits and primary concerns surrounding this energy source.\n Plot 5 - CO2 Emission Trends\n\n\n\n\n\n\n\n\nSource code for the above analysis: Github"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Energy and power are at the center of all aspects of modern civilization. Human society’s food, transportation, communication, infrastructure, and more are connected to and dependent on “the grid.” The resilience of that grid, the sources of power for that grid, and the ways in which our energy consumption ebbs and flows are at the forefront of technological development, public policy, and economic analysis and this study will delve into it all through the lens of statistical time series analysis.\nIt is well documented at this point, by the International Panel on Climate Change, most government organizations and top educational institutions, as well as the leading scientific journals in America, that the threat of climate change is dire, imminent, and will affect the most vulnerable populations in the country first. The prevalence and intensity of extreme weather events, such as severe drought and wildfires in the American West and major flooding events across the southeast, are the beginning of climate changes’ manifestations in America’s daily life. At the most recent global climate summit - COP 27 - it was agreed upon that many countries should aim to cut energy emissions by up to 57% in order to meet the goal of keeping warming under 1.5 degrees C. In order to do that, a thorough examination of the United States’ history of energy use, where our energy comes from, and what a clean & resilient energy future might look like is critical. This is the mission of this project.\nArticles/Literature to get you started:  - U.S. Energy Facts  - Assessment of threats to the American power grid - Energy Sources vs. Health Impacts\nImportant Questions::\n\nHow has overall energy consumption changed in the United States in the last 100 years?\nHow have energy sources shifted in the United States?\nHow has the cost of energy and electricity changed?\nHas the clean energy job market changed in the last 50 years relative to the fossil-fuel sector?\nCan we track major energy infrastructure projects in the United States and their current quality and resiliency status?\nCan we track funding for various energy projects - including clean vs. dirty energy?\nHow have carbon emissions relative to various energy sources evolved in recent years?\nWhat documented health impacts have there been associated with various energy sources?\nTo what extent can we document the United States’ reliance on domestic vs. foreign energy supplies in recent decades?\nHow do carbon markets impact the energy sector and how effective are they at offsetting climate change?\nWhat industries have seen the most growth in clean energy usage - and where will future growth likely come from?\n\nGitHub Repository for this Project can be found here!"
  },
  {
    "objectID": "datasources.html",
    "href": "datasources.html",
    "title": "Data Sources",
    "section": "",
    "text": "To address the guiding questions outlined on the Introduction Page, data was sourced from the U.S. Energy Information Administration (EIA), and the International Energy Agency (IEA) . As the project develops, more data may be added to this page.\n  U.S. Energy Information Administration\n1) Total Energy Usage By State\nThe EIA produces thousands of datasets spanning most facets of energy usage in the United States. The organization is one of the go-to sources for both historical data and energy forecast data, making their Open Data Portal a rich resource for this project. One dataset this project will examine is Net Generation by State by Type of Producer by Energy Source between 1990 and 2021. This will provide insight into how Coal, Hydroelectric, Natural Gas, Petroleum, Wind, and Wood fuel source consumption has evolved over the past 3 decades across all 50 states.\nThe data can be found here: Net Generation by State by Type of Producer by Energy Source\n\n2) Monthly CO2 Emissions from Coal\nThe EIA also has data on how many millions of metric tons of CO2 have been released from Coal since 1973. This is an important dataset for my analysis, as the analysis of US energy is directly tied to climate change and a push for cleaner fuel sources. As stated in the introduction, the world is at a critical point in determining our climate future. Analyzing how CO2 emissions, especially from one of the dirtiest fuel sources, have trended over time will be important in forecasting future emissions if we continue on a business as usual path.\nThe data can be sourced here: Table 11.1 - Carbon dioxide emissions from energy consumption by source \n\n3) Petroleum Exports vs. Imports\nAlso of interest is the United States’ energy dependence on foreign entities vs. reliance on domestic energy production. To dig into this the EIA provides an overview of Petroleum consumption, production, imports, and outputs starting in 1950 through 2020 to examine how the country’s relationship with foreign entities has evolved. Data is measured in thousands of barrels per day.\nData can be found here: Table 3.1 Petroleum Overview \n\n4) Cost of Energy\nThe final EIA dataset used examines how energy costs have changed between 1970 to 2010 across many types of energy sources. Downloaded directly from the OpenData portal, this dataset looks at Consumer Price Estimates for Energy by Source, 1970-2010 calculates the cost in dollars per million Btu for coal, natural gas, fuel oil, petroleum, nuclear fuel, and biomass annually. This data will provide insights into how affordable each fuel type has become, and what the cost trajectory for each looks like as the technology improves and we try to forecast into the future.\nData can be found here: Table 3.3 Consumer Price Estimates for Energy by Source, 1970-2010\n\n \n International Energy Agency\nThe International Energy Agency is an intergovernmental organization based in Paris that provides data-backed policy recommendations and analysis on questions of global energy. Founded in 1974, their areas of work are promoting energy efficiency, energy security, demand-driven electricity networks, and fostering international partnerships to achieve a clean and sustainable future. One of their datasets, regarding global investment in various energy sources, will provide critical insight into how billions of dollars flow through different fuel sources - either furthering or hampering their global usage.\nData can be found here: World Energy Investment 2022 Datafile"
  },
  {
    "objectID": "eda.html#dataset-1---electricity-production",
    "href": "eda.html#dataset-1---electricity-production",
    "title": "Exploratory Data Analysis",
    "section": "Dataset 1 - Electricity Production",
    "text": "Dataset 1 - Electricity Production\n\n1) Identify time series components of data\n It is first important to visualize the time series at its most basic level. Electricity production data has been subsetted to include just 4 energy sources: Coal, Nuclear, Wind, and Solar. Below is a basic time series plot of US production of these products between 1990 and 2022.\n\n\n\n\n\nThis preliminary examination of the data shows that production of nuclear, solar, and wind have trended upwards over this timeframe while coal peaked in 2007 and has trended down ever since. There isn’t much seasonality or cyclicality in production of nuclear, solar or wind but there may be some additive seasonality in coal production. Coal production, however, may be seasonal, as demand for coal spikes during cold winter months and drops during summer months. Further analysis and tools are necessary to confirm this hypothesis.\n\n\n2) Lag Plots\n Examining lags in time series data highlights any potential correlations between steps along the time interval –> i.e. is energy production at time t correlated with energy production at time t-1? Does the previous year’s energy production impact energy production in future years (e.g. perhaps because of investments made in infrastructure or the passage of legislation that would affect production). Lag plots also help to understand any potential seasonality in the data.\n\n\n\n\n1 Year, 2 Year, 10 Year, 20 Year Lags in Coal Production\n\n\n\n\n\n\n\n\n1 Year, 2 Year, 10, 20 Year Lags in Nuclear Production\n\n\n\n\n\n\n\n\n1 Year, 2 Year, 10, 20 Year Lags in Wind Production\n\n\n\n\n\n\n\n\n1 Year, 2 Year, 10, 20 Year Lags in Solar Production\n\n\n\n\n\n\nIn coal production, there’s a weak autocorrelation between the data and a 1 year lag, indicating that there is some connection between year to year production but other factors may play a larger role. Lags 2 and 10 are essentially random; thus, it’s likely that there isn’t a correlation between the time series data and lags farther away. Lag 20 shows a weak negative correlation - weak enough that we can’t make any grand-standing hypotheses to explain it.\nNuclear production shows no autocorrelation across any lag periods, while wind and solar exhibit strong autocorrelation for Lags 1 and 2 and no correlation for lags of 10 and 20. These strong correlations for 1 to 2 year lags are likely due to weather cycles - wind and solar are more dependent on weather systems such as El Niño or La Niña which exhibit their own cyclical behavior.\n\n\n\n3) Decompose data\n The data cannot be decomposed because it lacks seasonality! For more information on this data’s behavior, see the simple moving average (SMA) analysis below.\n\n\n\n\n\n4) ACF/PACF Plots \nAutocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots are useful for showing the correlation coefficients between a time series and its own lagged values. ACF looks at the time series and previous values of the series measured for different lag lenths - while PACF looks directly at the correlation between a time series and a given lag. Both are also useful for identifying if the data has stationarity - where the mean, variance, and autocorrelation structure do not change over time.\nBefore building ACF/PACF plots the data is de-trended - using detrending and differencing - to examine seasonality and cyclicality directly. Detrending occurs by regressing the total production output against the date, predicting total production output for each date, and finding the difference between prediction and actual outputs. Differencing occurs by subtracting the previous observation from the current observation to remove temporal dependence.\n\n\n\n\n\n\n\n\n\n\n\nThe original data was already fairly stationary - as de-trending the data didn’t change the lagged correlations at all. Correlations still exist for lags every 5 years (very cyclical), but the data has a constant mean and variance across time making it at least weakly stationary. Differencing was performed to see how the data would behave - and it was likely unnecessary given that the original data was already stationary.\n\n\n5) Augmented Dickey-Fuller Test\nADF is used to check that the data is stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  energy_sources$Output\nDickey-Fuller = -13.743, Lag order = 6, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe Augmented Dickey-Fuller tests are used to assess how stationary the data is. An ADF test of all energy production outputs produced a p-value of 0.01. Thus, we can reject the null hypothesis of no stationarity at the 1% level - indicating that this series does in fact exhibit stationarity. This aligns with findings from the ACF plots above. \n\n\n6) Moving Average Smoothing \nThis smoothing helps identify underlying patterns, especially because of the lack of seasonality in the data. Three different MA windows for smoothing are selected: 5,10,20.\nThe order of SMA tells us the number of data points used to calculate the moving average. Thus, a window of 5 will be more responsive to short-term fluctuations and exhibit more details of the trend over a shorter time frame. An order of 20 is far smoother, and less responsive to year to year events.\n\n\n\n\n\nTime elapsed: 0.22 seconds\nModel estimated using sma() function: SMA(5)\nDistribution assumed in the model: Normal\nLoss function type: MSE; Loss function value: 2.623999e+16\nARMA parameters of the model:\nAR:\nphi1[1] phi2[1] phi3[1] phi4[1] phi5[1] \n    0.2     0.2     0.2     0.2     0.2 \n\nSample size: 32\nNumber of estimated parameters: 3\nNumber of degrees of freedom: 29\nInformation criteria:\n     AIC     AICc      BIC     BICc \n1306.606 1307.463 1311.003 1312.488 \n\n\n\n\n\nTime elapsed: 0.21 seconds\nModel estimated using sma() function: SMA(10)\nDistribution assumed in the model: Normal\nLoss function type: MSE; Loss function value: 2.472111e+16\nARMA parameters of the model:\nAR:\n phi1[1]  phi2[1]  phi3[1]  phi4[1]  phi5[1]  phi6[1]  phi7[1]  phi8[1] \n     0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1 \n phi9[1] phi10[1] \n     0.1      0.1 \n\nSample size: 32\nNumber of estimated parameters: 3\nNumber of degrees of freedom: 29\nInformation criteria:\n     AIC     AICc      BIC     BICc \n1304.698 1305.555 1309.095 1310.581 \n\n\n\n\n\nTime elapsed: 0.53 seconds\nModel estimated using sma() function: SMA(20)\nDistribution assumed in the model: Normal\nLoss function type: MSE; Loss function value: 8.031173e+17\nARMA parameters of the model:\nAR:\n phi1[1]  phi2[1]  phi3[1]  phi4[1]  phi5[1]  phi6[1]  phi7[1]  phi8[1] \n    0.05     0.05     0.05     0.05     0.05     0.05     0.05     0.05 \n phi9[1] phi10[1] phi11[1] phi12[1] phi13[1] phi14[1] phi15[1] phi16[1] \n    0.05     0.05     0.05     0.05     0.05     0.05     0.05     0.05 \nphi17[1] phi18[1] phi19[1] phi20[1] \n    0.05     0.05     0.05     0.05 \n\nSample size: 32\nNumber of estimated parameters: 3\nNumber of degrees of freedom: 29\nInformation criteria:\n     AIC     AICc      BIC     BICc \n1416.085 1416.942 1420.482 1421.967 \n\n\nFrom this exploration, it seems that SMA with an order of 10 smoothing is the best for the data. The yearly data required a fairly small order for smoothing given the sparse nature of the data points. A window of 10 minimized the information criteria (AIC, BIC, AICc) and provided the best smoothing of data fluctuations and view into the underlying trends. From this we can see that the trend in electricity production is steadily increasing between 1990 -1995 before beginning a slow descent down to 2005."
  },
  {
    "objectID": "eda.html#dataset-2---petroleum-exports-vs.-imports",
    "href": "eda.html#dataset-2---petroleum-exports-vs.-imports",
    "title": "Exploratory Data Analysis",
    "section": "Dataset 2 - Petroleum Exports vs. Imports",
    "text": "Dataset 2 - Petroleum Exports vs. Imports\n\n1) Identify time series components of data\n Below is the unaltered time series. The data contains Petroleum Imports, Exports, and Total Field Production in thousands of barrels per day between 1990 and 2022.\n\n\n\n\n\nInitial examination of the data suggests that petroleum imports and exports both trended upwards, likley in response to energy usage and dependence in the U.S. increasing over the time interval. Total petroleum field production may also trend upwards - although there is more randomness to its behavior. Only annual data is available - hence seasonality cannot be surmised here, but there may be some slight cyclicality to petroleum exports. Furthermore, it’s plausible that petroleum imports may be tied to the economy and grow during bull and crash during bear markets; however, further examination is needed to assess this hypothesis.\n\n\n2) Lag Plots\nLag plots are again used to examine autocorrelation between the time series and itself - i.e. do current petroleum imports and exports impact future petroleum imports and exports?\n\n\n\n\n1 Year, 2 Year, 10 Year, 20 Year Lags in Petroleum Imports\n\n\n\n\n\n\n\n\n1 Year, 2 Year, 10 Year, 20 Year Lags in Petroleum Exports\n\n\n\n\n\n\n\n\n1 Year, 2 Year, 10 Year, 20 Year Lags in Petroleum Total Field Production\n\n\n\n\n\n\nPetroleum imports show weak autocorrelation with lags 1, 2 and 10 - weakening over time. By lag 20 there is no correlation between the time series and the lagged values (it’s just random). Petroleum exports show stronger autocorrelation with lags 1 and 2, while the lagged values at lags 10 and 20 are random and not correlated with the time series. Petroleum total field production shows no autocorrelation with itself across any lag period.\nWe can explain these results with intuition: Petroleum imports and exports are largely dependent on foreign policy, international crises, domestic policy affecting drilling permits, and supply and demand for petroleum. These external factors play a much larger role in determining imports and exports than previous years’ values of imports and exports. There is a weak correlation between lag 1 and 2 as often the same external factors (i.e. policy or crisis) are in play for multiple years and therefore closely grouped years exhibit similar behavior. But this explains why there is very little correlation with lagged period greater than 2 years.\n\n\n\n\n3) Decompose Data\n\n\n\nThe data cannot be decomposed because it does not exhibit seasonality. See SMA (Simple moving average) below for more information on its behavior.\n\n\n\n4) ACF/PACF Plots\n\n\nDetrending\nNone of the series exhibit stationarity so detrending must be undertaken for all three.\n\n\n[1] \"Original import data vs. detrended import data\"\n\n\n\n\n\n[1] \"Original export data vs. detrended export data\"\n\n\n\n\n\n[1] \"Original field production data vs. detrended field production data\"\n\n\n\n\n\nThe ACF and PACF plots for Imports, Exports, and Production Totals illustrate that the data does actually have some seasonality given the sinusoidal curves demonstrated across all three. Detrending imports eliminated autocorrelation between the time series and lags 2,3, and 4. Interestingly, detrending export and production did not significantly change the correlations across lags - which is especially bizarre given that the ADF Tests (below) return extremely high p values - meaning we strongly fail to reject the null hypothesis of no stationarity.\n\n\n\n5) Augmented Dickey-Fuller Test\nWe mathematically examine stationarity of Imports, Exports, and Production data using the ADF Test on detrended data below.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  resid(fit_import)\nDickey-Fuller = -2.7874, Lag order = 2, p-value = 0.2724\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  resid(fit_export)\nDickey-Fuller = 2.8814, Lag order = 2, p-value = 0.99\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  resid(fit_prod)\nDickey-Fuller = -0.60931, Lag order = 2, p-value = 0.9659\nalternative hypothesis: stationary\n\n\nEven after detrending the data does not exhibit stationarity via the ADF test! Thus, differencing is used to generate stationary data.\n\nDifferencing Imports, Exports, and Production –> ADF Test\nLog() transformations are employed on Exports and Production data due to the high degrees of non-stationarity.\n\n\nImports\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  import$Value %>% diff() %>% diff()\nDickey-Fuller = -3.6065, Lag order = 2, p-value = 0.04954\nalternative hypothesis: stationary\n\n\n\n\nExports\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(export$Value) %>% diff() %>% diff()\nDickey-Fuller = -4.5144, Lag order = 2, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(production$Value) %>% diff() %>% diff()\nDickey-Fuller = -4.0549, Lag order = 2, p-value = 0.02134\nalternative hypothesis: stationary\n\n\nFinally, after using log() transformations on Exports and Production data, and twice differencing all three series - stationarity is achieved. This data has a strong upwards trends and lack of cyclicality and seasonality - making it complicated to coerce into stationary behavior.\n\n\n\n\n6) Moving Average Smoothing\n\nThis smoothing helps identify underlying patterns, especially because of the lack of seasonality in the data. Three different MA windows for smoothing are selected: 3,5,10.\n\n\n\n\n\nTime elapsed: 0.03 seconds\nModel estimated using sma() function: SMA(3)\nDistribution assumed in the model: Normal\nLoss function type: MSE; Loss function value: 3087529\nARMA parameters of the model:\nAR:\nphi1[1] phi2[1] phi3[1] \n 0.3333  0.3333  0.3333 \n\nSample size: 24\nNumber of estimated parameters: 2\nNumber of degrees of freedom: 22\nInformation criteria:\n     AIC     AICc      BIC     BICc \n430.7382 431.3096 433.0943 434.0023 \n\n\n\n\n\nTime elapsed: 0.02 seconds\nModel estimated using sma() function: SMA(5)\nDistribution assumed in the model: Normal\nLoss function type: MSE; Loss function value: 4514177\nARMA parameters of the model:\nAR:\nphi1[1] phi2[1] phi3[1] phi4[1] phi5[1] \n    0.2     0.2     0.2     0.2     0.2 \n\nSample size: 24\nNumber of estimated parameters: 2\nNumber of degrees of freedom: 22\nInformation criteria:\n     AIC     AICc      BIC     BICc \n439.8547 440.4261 442.2108 443.1188 \n\n\n\n\n\nTime elapsed: 0.03 seconds\nModel estimated using sma() function: SMA(10)\nDistribution assumed in the model: Normal\nLoss function type: MSE; Loss function value: 7834077\nARMA parameters of the model:\nAR:\n phi1[1]  phi2[1]  phi3[1]  phi4[1]  phi5[1]  phi6[1]  phi7[1]  phi8[1] \n     0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1 \n phi9[1] phi10[1] \n     0.1      0.1 \n\nSample size: 24\nNumber of estimated parameters: 2\nNumber of degrees of freedom: 22\nInformation criteria:\n     AIC     AICc      BIC     BICc \n453.0849 453.6563 455.4410 456.3490 \n\n\nSMA was again used to remove noise and identify trends in the petroleum exports vs imports data. From examination of a few different SMA windows (3,5,10), order 5 is selected. From this it’s clear that petroleum imports and exports have trended up throughout the time period of examination and the major spikes and dips in production have been smoothed out."
  },
  {
    "objectID": "eda.html#dataset-3---cost-of-energy",
    "href": "eda.html#dataset-3---cost-of-energy",
    "title": "Exploratory Data Analysis",
    "section": "Dataset 3 - Cost of Energy",
    "text": "Dataset 3 - Cost of Energy\nFor this dataset, I examine the total cost of energy across multiple sources: coal, natural gas, biomass, petroleum, wind, etc.\n\n\n\n\n1) Identify time series components of data\n\n\n\n\n\n\nThe basic time series plot of total cost of electricity in the US shows some interesting patterns. There’s a definite upwards trend due to inflation and rising cost of goods and services, as well as some fluctuations that appear to be cyclical. It doesn’t look like there’s much seasonality in this data but the cyclicality will be examined.\nThe cost of electricity had a local peak in 1983 - during the oil crisis in the US - and then again began to rise exponentially in the early 2000s. This coincided with the US’ maximum petroleum imports and the rise of natural gas as the dominent energy source. The cost of electricity dropped in 2008 due to the global financial crisis, and then steadily rose until the end of the timeframe examined (2010).\n\n\n2) Lag Plots\n\n\n\n\n\n1 Year, 2 Year, 10 Year, 20 Year Lags in Petroleum Imports\n\n\n\n\n\n\nThere is a weak correlation between the time series and lag 1, and an even weaker correlation with lag 2. Values at lags 10 and 20 don’t appear to be correlated with the time series at all. This is somewhat surprising, as the cost of electricity is tied to external factors (such as global fuel supply) but prices are also tied to the US economy which is highly cyclical. I expected the 10 and 20 year lags to be correlated to the time series due to economic cycles but that does not appear in the data.\n\n\n\n3) Decompose data\n\n\n\n\n\n\nThrough decomposition of the time series, a more concrete idea of the trend and seasonality appears. The data is indeed trending upwards, with a spike in the early 2000s and a crash during the 2008 financial crisis. There is some seasonal variation - and it is certainly additive based on the decomposition plot.\n\n\n4) ACF/PACF Plots\n\nAgain, this data is certainly not stationary so de-trending is potentially helpful. ACF/PACF plots are compared for both the original and de-trended data.\n\n\n[1] \"Original electricity cost data vs. detrended electricity cost data\"\n\n\n\n\n\nThe data is clearly not stationary, as there is a gradual weakening of the correlation with lagged values until lag 10, and then a large spike of negatively correlated lagged values around lags 30-40. This negative correlation is reflective of the strong upward trend over time as well as the exponential price growth in the early 2000s. The mean, variance, and autocorrelation pattern are not consistent over time and therefore the data does not have stationarity.\nTo address this, I tried basic detrending mechanisms, but this did not coerce the data into stationarity. The ADF Test below confirms this. Thus, differencing was undertaken to achieve the desired stationary behavior.\n\n\n5) Augmented Dickey-Fuller Test\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  resid(fit_cost)\nDickey-Fuller = -2.4232, Lag order = 3, p-value = 0.4071\nalternative hypothesis: stationary\n\n\nWith a p-value of 0.4071, we fail to reject the null hypothesis that the detrended data doesn’t have stationarity.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log(import$Value) %>% diff()\nDickey-Fuller = -3.7192, Lag order = 2, p-value = 0.04149\nalternative hypothesis: stationary\n\n\nIn order to coerce the cost of electricity data into having stationary properties, the data is transformed via log() and then differenced once. After transformation and differencing, the ACF plot indicates no lags that are statistically significantly correlated with the time series and the mean, variance, and autocorrelation function behavior are constant over time.\nAdditionally, now the ADF test returns a p value of 0.04149 which is statistically significant at the 5% level. Thus, we can now reject the null hypothesis that the data is not stationary and proceed with future modeling.\n\n\n\n6) Moving Average Smoothing\n\nThis smoothing helps identify underlying patterns, especially because of the lack of seasonality in the data. Three different MA windows for smoothing are selected: 3,5,10.\n\n\n\n\n\nTime elapsed: 0 seconds\nModel estimated using sma() function: SMA(5)\nDistribution assumed in the model: Normal\nLoss function type: MSE; Loss function value: 1.8573\nARMA parameters of the model:\nAR:\nphi1[1] phi2[1] phi3[1] phi4[1] phi5[1] \n    0.2     0.2     0.2     0.2     0.2 \n\nSample size: 41\nNumber of estimated parameters: 0\nNumber of degrees of freedom: 41\nInformation criteria:\n     AIC     AICc      BIC     BICc \n141.7371 141.7371 141.7371 141.7371 \n\n\n\n\n\nTime elapsed: 0.01 seconds\nModel estimated using sma() function: SMA(10)\nDistribution assumed in the model: Normal\nLoss function type: MSE; Loss function value: 3.6406\nARMA parameters of the model:\nAR:\n phi1[1]  phi2[1]  phi3[1]  phi4[1]  phi5[1]  phi6[1]  phi7[1]  phi8[1] \n     0.1      0.1      0.1      0.1      0.1      0.1      0.1      0.1 \n phi9[1] phi10[1] \n     0.1      0.1 \n\nSample size: 41\nNumber of estimated parameters: 0\nNumber of degrees of freedom: 41\nInformation criteria:\n     AIC     AICc      BIC     BICc \n169.3311 169.3311 169.3311 169.3311 \n\n\n\n\n\nTime elapsed: 0.01 seconds\nModel estimated using sma() function: SMA(15)\nDistribution assumed in the model: Normal\nLoss function type: MSE; Loss function value: 4.8375\nARMA parameters of the model:\nAR:\n phi1[1]  phi2[1]  phi3[1]  phi4[1]  phi5[1]  phi6[1]  phi7[1]  phi8[1] \n  0.0667   0.0667   0.0667   0.0667   0.0667   0.0667   0.0667   0.0667 \n phi9[1] phi10[1] phi11[1] phi12[1] phi13[1] phi14[1] phi15[1] \n  0.0667   0.0667   0.0667   0.0667   0.0667   0.0667   0.0667 \n\nSample size: 41\nNumber of estimated parameters: 0\nNumber of degrees of freedom: 41\nInformation criteria:\n     AIC     AICc      BIC     BICc \n180.9856 180.9856 180.9856 180.9856 \n\n\nSMA was again used to remove noise and identify trends in the cost of energy. From examination of a few different SMA windows (5,10,15), order 10 is selected. From this it’s clear that electricity prices have trended up throughout the time period of examination and the major spikes and dips in production have been smoothed out."
  },
  {
    "objectID": "arma_models.html#dataset-1---electricity-production",
    "href": "arma_models.html#dataset-1---electricity-production",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Dataset 1 - Electricity Production",
    "text": "Dataset 1 - Electricity Production\n\n\n\n\n1) Data Review\nFrom EDA, we looked at ACF graphs and checked ADF to see if the data was stationary. The data was not stationary so we’ll take the log to try to make it at least weakly stationary.\nLog Transform energy production outputs - and plot\n\n\n\n\n\nCheck if data is now stationary using ACF plot\n\n\n\n\n\nIt’s now weakly stationary! No further differencing necessary.\n\n\n2) ACF/PACF Plots\nACF/PACF plots are used to select potential p and q values for an ARMA(p,q) model. ARMA is used because no differencing of the data occurred. From these plots, some potential p and q values are selected to fit an ARMA model with. Separate models have to be fit for each energy source, thus separate ACF/PACF plots are shown below.\nCoal\n\n\n\n\n\nMoving average order (found from ACF): q - 1,2,3,4  Autoregressive term (found from pACF): p - 1\nNuclear\n\n\n\n\n\nMoving average order (found from ACF): q - 1,2,3,4,5  Autoregressive term (found from pACF): p - 1\nWind\n\n\n\n\n\nMoving average order (found from ACF): q - 1,2,3,4,5 Autoregressive term (found from pACF): p - 1\nSolar\n\n\n\n\n\nMoving average order (found from ACF): q - 1,2,3  Autoregressive term (found from pACF): p - 1\n\n\n3) Fit ARIMA(p,d,q) models\nCoal\n\n\n\n\n\np\nq\nAIC\n\n\n\n\n1\n1\n1319.547\n\n\n1\n2\n1325.199\n\n\n1\n3\n1312.109\n\n\n1\n4\n1313.924\n\n\n\n\n\nNuclear\n\n\n\n\n\np\nq\nAIC\n\n\n\n\n1\n1\n1213.296\n\n\n1\n2\n1212.388\n\n\n1\n3\n1214.309\n\n\n1\n4\n1225.694\n\n\n1\n5\n1229.118\n\n\n\n\n\nWind\n\n\n\n\n\np\nq\nAIC\n\n\n\n\n1\n1\n1144.565\n\n\n1\n2\n1147.529\n\n\n1\n3\n1148.876\n\n\n1\n4\n1147.240\n\n\n1\n5\n1150.403\n\n\n\n\n\nSolar\n\n\n\n\n\np\nq\nAIC\n\n\n\n\n1\n1\n1069.721\n\n\n1\n2\n1063.944\n\n\n1\n3\n1073.950\n\n\n\n\n\n\n\n4) Model Diagnostics\nWith the above exploration, a best fit model is chosen for each energy source by minimizing AIC.\nCoal\nThe ARMA(p,q) model that minimizes AIC is ARMA(1,3).\n\n\n\n\n\n\nCall:\narma(x = coal_ts, order = c(1, 3))\n\nModel:\nARMA(1,3)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-467564239 -111368527   52069246  112738236  326924565 \n\nCoefficient(s):\n            Estimate  Std. Error    t value Pr(>|t|)    \nar1        1.074e+00   9.793e-03    109.664   <2e-16 ***\nma1       -3.294e-01   2.462e-01     -1.338   0.1810    \nma2       -3.552e-02   6.203e-01     -0.057   0.9543    \nma3        5.782e-01   2.625e-01      2.203   0.0276 *  \nintercept -2.978e+08   1.120e+04 -26602.741   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFit:\nsigma^2 estimated as 2.75e+16,  Conditional Sum-of-Squares = 7.701038e+17,  AIC = 1312.11\n\n\nARMA(1,3) for coal produces an AIC value of 1312.11 and a CSS value of 7.7x10^17. Using the model summary and plot of the model against the data above, it seems that the model fits the data pretty well but not perfectly. The randomness and sensitivity of the original data makes it hard for the model to truly fit well without overfitting. To achieve a better fit would require overfitting the training data and wouldn’t generalize well to new data.\n\\[\n\\phi(B) = 1 - 1.07(B)\n\\] \\[\n\\theta(B) = 1 + 3.29*10^-1 *(B) + 3.552*10^-2*(B^2) - 5.782*10^-1*(B^3)\n\\]\nNuclear\nThe ARMA(p,q) model that minimizes AIC is ARMA(1,2).\n\n\n\n\n\n\nCall:\narma(x = nuclear_ts, order = c(1, 2))\n\nModel:\nARMA(1,2)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-114143177  -26807890    5091034   22515762   52763712 \n\nCoefficient(s):\n            Estimate  Std. Error   t value Pr(>|t|)    \nar1        8.976e-01   3.176e-03   282.600   <2e-16 ***\nma1        5.650e-02   1.914e-01     0.295   0.7678    \nma2       -3.256e-01   1.744e-01    -1.868   0.0618 .  \nintercept  1.657e+08   7.670e+03 21598.292   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFit:\nsigma^2 estimated as 1.298e+15,  Conditional Sum-of-Squares = 3.764894e+16,  AIC = 1212.39\n\n\nARMA(1,2) for nuclear produces an AIC value of 1212.39 and a CSS value of 3.76x10^16. It seems that the model fits the data pretty well but not perfectly. To achieve a better fit would require overfitting the training data and wouldn’t generalize well to new data.\n\\[\n\\phi(B) = 1 - 8.976*10^-1(B)\n\\] \\[\n\\theta(B) = 1 - 5.65*10^-2 *(B) + 3.256*10^-1*(B^2)\n\\]\nWind\nThe ARMA(p,q) model that minimizes AIC is ARMA(1,1).\n\n\n\n\n\n\nCall:\narma(x = wind_ts, order = c(1, 1))\n\nModel:\nARMA(1,1)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-22040549  -6078188  -4049714   3768737  33558013 \n\nCoefficient(s):\n           Estimate  Std. Error  t value Pr(>|t|)    \nar1       1.105e+00   1.202e-02   91.912   <2e-16 ***\nma1       4.621e-01   1.890e-01    2.444   0.0145 *  \nintercept 6.648e+06   3.527e+04  188.479   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFit:\nsigma^2 estimated as 1.659e+14,  Conditional Sum-of-Squares = 4.976633e+15,  AIC = 1144.57\n\n\nARMA(1,1) for wind produces an AIC value of 1144.57 and a CSS value of 4.97x10^15. The model fits this data much better than the previous data as it follows a steadier upward trend over time as wind production has been more widely adopted by US states. Additionally, wind power is less prone to political shocks so is easier to predict and fit.\n\\[\n\\phi(B) = 1 - 1.05(B)\n\\] \\[\n\\theta(B) = 1 + 4.62*10^-1 *(B)\n\\]\nSolar\nThe ARMA(p,q) model that minimizes AIC is ARMA(1,2).\n\n\n\n\n\n\nCall:\narma(x = solar_ts, order = c(1, 2))\n\nModel:\nARMA(1,2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-8923843   -53422   331446  1505395 10832340 \n\nCoefficient(s):\n            Estimate  Std. Error  t value Pr(>|t|)    \nar1        1.268e+00   1.864e-02   68.028  < 2e-16 ***\nma1        1.018e+00   1.741e-01    5.849 4.96e-09 ***\nma2       -2.671e-01   2.142e-01   -1.247    0.212    \nintercept -5.627e+05         NaN      NaN      NaN    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nFit:\nsigma^2 estimated as 1.255e+13,  Conditional Sum-of-Squares = 3.82509e+14,  AIC = 1063.94\n\n\nSimiarly, ARMA(1,2) for solar produces an AIC value of 1,063.94 and a CSS value of 3.82x10^9. The model fits this data much better than the previous data as it follows a steadier upward trend over time as solar production has been more widely adopted by US states. Additionally, solar power is less prone to political shocks so is easier to predict and fit.\n\\[\n\\phi(B) = 1 - 1.268(B)\n\\] \\[\n\\theta(B) = 1 - 1.018(B) + 2.671*10^-1*(B^2)\n\\]\n\n\n5) Fit an ARMA(p,q) model using auto.arima()\nCoal\n\n\nSeries: coal_ts \nARIMA(2,2,2) \n\nCoefficients:\n          ar1      ar2      ma1     ma2\n      -0.2910  -0.8112  -0.7750  0.8498\ns.e.   0.1896   0.1227   0.2165  0.3065\n\nsigma^2 = 2.109e+16:  log likelihood = -606.73\nAIC=1223.45   AICc=1225.95   BIC=1230.46\n\n\nAuto.arima() chooses AR(2,2) as the best model, very different from my chosen model of AR(1,3). This is likely because auto.arima() considers a more comprehensive set of parameters (i.e. I didn’t check p=2 as a possible value) and I chose my model based on AIC alone whereas auto.arima() chooses models based on AIC, BIC, and AICc.\nNuclear\n\n\nSeries: nuclear_ts \nARIMA(0,1,0) with drift \n\nCoefficients:\n         drift\n      12988794\ns.e.   7504730\n\nsigma^2 = 1.79e+15:  log likelihood = -587.85\nAIC=1179.71   AICc=1180.13   BIC=1182.57\n\n\nThe model chosen here is (0,1,0) which is bizarre. This only applies differencing, but doesn’t use AutoRegressive modeling or Moving Average modeling. This is likely because the data is hard to fit.\nWind\n\n\nSeries: wind_ts \nARIMA(0,2,1) \n\nCoefficients:\n          ma1\n      -0.4339\ns.e.   0.1699\n\nsigma^2 = 2.172e+14:  log likelihood = -537.34\nAIC=1078.68   AICc=1079.13   BIC=1081.49\n\n\nThe model chosen is ARIMA(0,2,1), which is essentially an MA() model with differencing. The search algorithm decided to difference twice, where I didn’t because I already saw weak stationarity in initial exploration.\nSolar\n\n\nSeries: solar_ts \nARIMA(0,2,1) \n\nCoefficients:\n         ma1\n      0.8289\ns.e.  0.1120\n\nsigma^2 = 2.766e+13:  log likelihood = -506.9\nAIC=1017.81   AICc=1018.25   BIC=1020.61\n\n\nThe model chosen is ARIMA(0,2,1), which is essentially an MA() model with differencing. The search algorithm decided to difference twice, where I didn’t because I already saw weak stationarity in initial exploration.\n\n\n6) Forecast\nForecast each energy source for the next three years - using manually selected models, but including 2 orders of differencing for each.\nCoal\n\n\n\n\n\nForecasting coal production three years into the future, it’s expected to continue declining along the lines of its current downward trend. There’s quite a large confidence band around this estimate, as many other factors will affect coal production - such as the geopolitical climate surrounding coal.\nNuclear\n\n\n\n\n\nForecasting nuclear production three years into the future, it’s expected to stay fairly steady along the 1.5x10^9 tons line. There’s quite a large confidence band around this estimate, as many other factors will affect wind production - especially political sentiment around its usage.\nWind\n\n\n\n\n\nForecasting wind production three years into the future, it’s expected to continually exponentially growing as per its existing trend. There’s a much smaller confidence band around this esimate, as its seen a steady increase over this timeframe. Infrastructure improvements in the space will affect the estimate greatly.\nSolar\n\n\n\n\n\nForecasting solar production three years into the future, it’s expected to continually exponentially growing as per its existing trend. There’s a much smaller confidence band around this esimate, as its seen a steady increase over this timeframe. Infrastructure improvements in the space will affect the estimate greatly.\n\n\n7) Benchmark Methods\nCompare the models to baseline benchmark methods to ensure they’re performing above.\nCoal\n\n\n\n\n\nNuclear\n\n\n\n\n\nWind\n\n\n\n\n\nSolar\n\n\n\n\n\nNote: Across all four energy sources, the fitted model does perform better than benchmark methods."
  },
  {
    "objectID": "arma_models.html#dataset-2---petroleum-exports-vs.-imports",
    "href": "arma_models.html#dataset-2---petroleum-exports-vs.-imports",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Dataset 2 - Petroleum Exports vs. Imports",
    "text": "Dataset 2 - Petroleum Exports vs. Imports\n\n\n\n\n1) Data Review\nFrom EDA, we looked at ACF graphs and checked ADF to see if the data was stationary. The data was not stationary so we take the log and difference to make it stationary.\nLog Transform petroleum imports, exports, and production - and plot\n\n\n\n\n\nSince the data is still not stationary, exports, imports, and production will be differenced.\nImports\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  import_ts %>% diff()\nDickey-Fuller = -3.7192, Lag order = 2, p-value = 0.04149\nalternative hypothesis: stationary\n\n\nExports\nMust be differenced twice. A first difference results in a p-value of 0.32 by the ADF test: indicating that we can’t reject the null hypothesis of no stationarity. Stationarity is only achieved with a second difference.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  export_ts %>% diff() %>% diff()\nDickey-Fuller = -4.5144, Lag order = 2, p-value = 0.01\nalternative hypothesis: stationary\n\n\nProduction Must be differenced twice. A first difference results in a p-value of 0.99 by the ADF test: indicating that we can’t reject the null hypothesis of no stationarity. Stationarity is only achieved with a second difference.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  production_ts %>% diff() %>% diff()\nDickey-Fuller = -4.0549, Lag order = 2, p-value = 0.02134\nalternative hypothesis: stationary\n\n\nAll three variables were differenced once or twice in order to achieve stationarity. The stationary property of the data is confirmed by the ACF plots of each, as well as the Augmented-Dickey-Fuller Test of the differenced series.\n\n\n2) ACF/PACF Plots\nACF/PACF plots are used to select potential p and q values for an ARIMA(p,d,q) model. From these plots, some potential p and q values are selected to fit an ARIMA model with. Separate models have to be fit for each variable thus separate ACF/PACF plots are shown below.\nImport\n\n\n\n\n\nMoving average order (found from ACF): q - 1,2,3  Autoregressive term (found from pACF): p - 1\nExports\n\n\n\n\n\nMoving average order (found from ACF): q - 1,2,3  Autoregressive term (found from pACF): p - 1\nProduction\n\n\n\n\n\nMoving average order (found from ACF): q - 1  Autoregressive term (found from pACF): p - 1\n\n\n3) Fit ARIMA(p,d,q) models\nImports\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n1.764891\n5.171374\n3.028049\n\n\n1\n2\n1\n1.782784\n5.055911\n3.116117\n\n\n1\n3\n1\n10.774584\n13.908151\n12.186349\n\n\n1\n1\n2\n3.519834\n8.061811\n5.742056\n\n\n1\n2\n2\n3.708435\n8.072605\n6.061376\n\n\n1\n3\n2\n7.919693\n12.097783\n10.419693\n\n\n1\n1\n3\n3.394707\n9.072178\n6.924119\n\n\n1\n2\n3\n5.588815\n11.044027\n9.338815\n\n\n1\n3\n3\n8.751611\n13.974223\n12.751611\n\n\n\n\n\nExports\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n1\n10.408080\n13.81456\n11.67124\n\n\n1\n2\n1\n11.644777\n14.91790\n12.97811\n\n\n1\n3\n1\n19.463857\n22.59742\n20.87562\n\n\n1\n1\n2\n9.090324\n13.63230\n11.31255\n\n\n1\n2\n2\n12.512548\n16.87672\n14.86549\n\n\n1\n3\n2\n21.466731\n25.64482\n23.96673\n\n\n1\n1\n3\n11.089958\n16.76743\n14.61937\n\n\n1\n2\n3\n11.108577\n16.56379\n14.85858\n\n\n1\n3\n3\n18.917698\n24.14031\n22.91770\n\n\n\n\n\nProduction\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n1\n1\n0\n-42.47036\n-40.19937\n-41.87036\n\n\n1\n2\n0\n-40.83047\n-38.64838\n-40.19889\n\n\n1\n3\n0\n-29.22643\n-27.13739\n-28.55977\n\n\n1\n1\n1\n-40.55816\n-37.15168\n-39.29500\n\n\n1\n2\n1\n-38.89113\n-35.61800\n-37.55780\n\n\n1\n3\n1\n-33.56462\n-30.43105\n-32.15285\n\n\n2\n1\n0\n-40.56932\n-37.16284\n-39.30617\n\n\n2\n2\n0\n-38.93721\n-35.66408\n-37.60387\n\n\n2\n3\n0\n-28.70668\n-25.57311\n-27.29491\n\n\n2\n1\n1\n-38.76965\n-34.22767\n-36.54743\n\n\n2\n2\n1\n-37.03220\n-32.66803\n-34.67926\n\n\n2\n3\n1\n-32.16038\n-27.98229\n-29.66038\n\n\n\n\n\n\n\n4) Model Diagnostics\nWith the above exploration, a best fit model for each variable is chosen by minimizing AIC, AICc, and BIC\nImports\nThe ARIMA model that minimizes AIC and AICc is ARIMA(1,1,1)\n\n\n\n\n\n\nCall:\narima(x = import_ts, order = c(1, 1, 1))\n\nCoefficients:\n         ar1      ma1\n      0.5586  -0.1272\ns.e.  0.4354   0.5220\n\nsigma^2 estimated as 0.04819:  log likelihood = 2.12,  aic = 1.76\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.05626467 0.2149074 0.1541133 0.7196743 1.833747 0.7871504\n                   ACF1\nTraining set -0.0981568\n\n\nWith this model, we get a training RMSE of 0.215, so the model fits the training data fairly well without overfitting due to its bizarre behavior.\n\\[\n\\phi(B) = 1 - 0.5586(B)\n\\] \\[\n\\theta(B) = 1 - 0.4354(B)\n\\]\nExports\nThe consensus across AIC, BIC, and AICc is that the best fit model is ARIMA(1,1,2).\n\n\n\n\n\n\nCall:\narima(x = export_ts, order = c(1, 1, 2))\n\nCoefficients:\n          ar1     ma1     ma2\n      -0.8486  1.7381  0.9117\ns.e.   0.1701  0.3518  0.3438\n\nsigma^2 estimated as 0.05395:  log likelihood = -0.55,  aic = 9.09\n\nTraining set error measures:\n                     ME      RMSE       MAE       MPE     MAPE      MASE\nTraining set 0.07329069 0.2273772 0.1747664 0.9874671 2.719147 0.7576492\n                   ACF1\nTraining set -0.0979957\n\n\n\\[\n\\phi(B) = 1 + 0.8486(B)\n\\] \\[\n\\theta(B) = 1 - 1.7101(B) - 0.9117(B^2)\n\\] With this model, we get a training RMSE of 0.2273. As evidenced by the plot, the model does a pretty terrible job with this model fit - likely due to the complex adn random nature of the data. Let’s see what auto.arima() would have chosen for this model.\nProduction\nThe consensus across AIC, BIC, and AICc is that the best fit model is ARIMA(1,1,0).\n\n\n\n\n\n\nCall:\narima(x = production_ts, order = c(1, 1, 0))\n\nCoefficients:\n         ar1\n      0.8427\ns.e.  0.1421\n\nsigma^2 estimated as 0.007357:  log likelihood = 23.24,  aic = -42.47\n\nTraining set error measures:\n                     ME       RMSE        MAE       MPE     MAPE      MASE\nTraining set 0.01526092 0.08398331 0.06473024 0.1645849 0.705565 0.7023005\n                    ACF1\nTraining set 0.008829977\n\n\n\\[\n\\phi(B) = 1 - 0.8427(B)\n\\]\nWith this model, we get a training RMSE of 0.08398331, so the model fits the training data fairly well without overfitting due to its bizarre behavior.\n\n\n5) Fit an ARIMA(p,d,q) model using auto.arima()\nImports\n\n\nSeries: import_ts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.1189\ns.e.  0.0450\n\nsigma^2 = 0.04869:  log likelihood = 2.63\nAIC=-1.26   AICc=-0.66   BIC=1.01\n\n\nThe model chosen is ARIMA(0,1,0) with drift. While auto.arima() agrees with my choice of differencing, it chose different parameters for p and q, and experimented with drift whereas I did not.\nExports\n\n\nSeries: export_ts \nARIMA(0,1,1) with drift \n\nCoefficients:\n         ma1   drift\n      0.4822  0.1498\ns.e.  0.1514  0.0766\n\nsigma^2 = 0.06913:  log likelihood = -1\nAIC=7.99   AICc=9.26   BIC=11.4\n\n\nThe model chosen is ARIMA(0,1,1) with drift. This is slightly different than my choice, likely because auto.arima() experimented with a wider range of parameters and experimented with drift.\nProduction\n\n\nSeries: production_ts \nARIMA(2,0,0) with non-zero mean \n\nCoefficients:\n         ar1      ar2    mean\n      1.7223  -0.8478  9.1494\ns.e.  0.1344   0.1229  0.1388\n\nsigma^2 = 0.007907:  log likelihood = 23.34\nAIC=-38.69   AICc=-36.58   BIC=-33.98\n\n\nThe model chosen is ARIMA(2,0,0) with non-zero mean - or an AR(2) model. AR is more suitable when the data has a clear pattern of autoregression and less randomness. This potentially makes sense given the lack of seasonality and large overall upward trend in production since 1970.\n\n\n6) Forecast\nForecasting petroleum imports, exports, and production for the next three years.\n\n\n\n\n\nAll three forecasts have a wide confidence band - indicating that based on past data alone we cannot generate a good estimate with high confidence. Imports are expected to increase slightly in the next 3 years, but have mostly stabilized. Exports are expected to increase for a year and then plateau/decrease for the next two years. Production is expected to continue exponentially increasing during the next three years.\n\n\n7) Benchmark\n\n\n\n\n\nThe models perform better than most of the models, expect the drift model. It’s clear that the models would benefit from incorporating a drift term to account for the intense drift especially production and exports suffer from."
  },
  {
    "objectID": "arma_models.html#dataset-3---cost-of-energy",
    "href": "arma_models.html#dataset-3---cost-of-energy",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Dataset 3 - Cost of Energy",
    "text": "Dataset 3 - Cost of Energy\n\n\n\n\n1) Data Review\nFrom EDA, we looked at ACF graphs and checked ADF to see if the data was stationary. The data was not stationary so we take the log and difference to make it stationary.\nLog Transform - and plot\n\n\n\n\n\nTaking the log of the data barely changed it! We’ll have to difference it. According to the ADF test, the data requires 2 differences. We can experiment with this during model building.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  cost_ts %>% diff()\nDickey-Fuller = -2.6121, Lag order = 3, p-value = 0.3332\nalternative hypothesis: stationary\n\n\n\n\n2) ACF/PACF Plots\nACF/PACF plots are used to select potential p and q values for an ARIMA(p,d,q) model. From these plots, some potential p and q values are selected to fit an ARIMA model with. Separate models have to be fit for each variable thus separate ACF/PACF plots are shown below.\nImport\n\n\n\n\n\nMoving average order (found from ACF): q - 1,2,3,4,5  Autoregressive term (found from pACF): p - 0,1\n\n\n3) Fit ARIMA(p,d,q) models\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n2\n1\n-39.93109\n-36.60397\n-39.59776\n\n\n0\n3\n1\n-17.18553\n-13.91036\n-16.84267\n\n\n0\n2\n2\n-38.52660\n-33.53591\n-37.84088\n\n\n0\n3\n2\n-29.18752\n-24.27477\n-28.48164\n\n\n0\n2\n3\n-36.60530\n-29.95105\n-35.42883\n\n\n0\n3\n3\n-27.35465\n-20.80430\n-26.14253\n\n\n0\n2\n4\n-37.89350\n-29.57569\n-36.07532\n\n\n0\n3\n4\n-26.43893\n-18.25100\n-24.56393\n\n\n0\n2\n5\n-36.23746\n-26.25609\n-33.61246\n\n\n0\n3\n5\n-26.28811\n-16.46259\n-23.57843\n\n\n1\n2\n1\n-38.52145\n-33.53077\n-37.83574\n\n\n1\n3\n1\n-22.09809\n-17.18533\n-21.39221\n\n\n1\n2\n2\n-39.77465\n-33.12040\n-38.59818\n\n\n1\n3\n2\n-27.31762\n-20.76727\n-26.10550\n\n\n1\n2\n3\n-37.90967\n-29.59186\n-36.09149\n\n\n1\n3\n3\n-28.53833\n-20.35040\n-26.66333\n\n\n1\n2\n4\n-36.48416\n-26.50279\n-33.85916\n\n\n1\n3\n4\n-27.12777\n-17.30225\n-24.41809\n\n\n1\n2\n5\n-34.48427\n-22.83934\n-30.87137\n\n\n1\n3\n5\n-25.13525\n-13.67215\n-21.40192\n\n\n\n\n\n\n\n4) Model Diagnostics\nWith the above exploration, a best fit model for each variable is chosen by minimizing AIC, AICc, and BIC\nThe ARIMA model that minimizes AIC, BIC, and AICc is ARIMA(0,2,1)\n\n\n\n\n\n\nCall:\narima(x = cost_ts, order = c(0, 2, 1))\n\nCoefficients:\n          ma1\n      -0.8515\ns.e.   0.1333\n\nsigma^2 estimated as 0.01836:  log likelihood = 21.97,  aic = -39.93\n\nTraining set error measures:\n                       ME      RMSE        MAE        MPE     MAPE      MASE\nTraining set -0.007565158 0.1321637 0.09492191 -0.1831388 7.305725 0.9102723\n                   ACF1\nTraining set 0.06219478\n\n\nWith this model, we get a training RMSE of 0.243, so the model fits the training data fairly well without overfitting. The random bumps and dips in the data have been largely smoothed, while the trend line is maintained.\n\\[\n\\theta(B) = 1 + 0.8616(B)\n\\]\n\n\n5) Fit an ARIMA(p,d,q) model using auto.arima()\n\n\nSeries: cost_ts \nARIMA(0,1,0) with drift \n\nCoefficients:\n       drift\n      0.0572\ns.e.  0.0207\n\nsigma^2 = 0.01756:  log likelihood = 24.6\nAIC=-45.19   AICc=-44.87   BIC=-41.81\n\n\nThe model chosen by auto.arima() is ARIMA(0,1,0) with drift. This is similar to the model parameters that I manually chose - except that auto.arima() chose to perform a second difference of the data and didn’t see the need for a moving average term.\n\n\n6) Forecast\nForecasting electricity costs for the next three years.\n\n\n\n\n\nAccording to our model, electricity costs are going to continue to rise over the next three years. Though there is ia decently large confidence band around the estimate, it is predicted that the slight dip in electricity costs during the Great Recession in 2008 were short lived and the market is going to recover. Since this data is old, I can confirm that electricity costs have continued to rise since the dataset stopped collecting data and the model forecast was correct.\n\n\n7) Benchmark\n\n\n\n\n\nThe model performs better than all the benchmark methods - except drift which performs about the same as the model. Again, this model could be improved by including a drift term and then should outperform even the drift benchmark."
  },
  {
    "objectID": "eda.html#dataset-4---co2-emissions",
    "href": "eda.html#dataset-4---co2-emissions",
    "title": "Exploratory Data Analysis",
    "section": "Dataset 4 - CO2 Emissions",
    "text": "Dataset 4 - CO2 Emissions\nFor this dataset, I examine CO2 emissions by month since 1973. I focus on 4 emissions sources: Coal, Hydrocarbon Gas, Natural Gas, and Petroleum.\n\n\n\n\n1) Identify time series components of data\n\n\n\n\n\n\nThe basic time series plot of total CO2 emissions by source shows some interesting patterns. Emissions from all four sources exhibit seasonality, probably because people use more fuel in the summer when it’s hot and ACs are running. Coal saw an upward trend from 1973-2007, and has since trended downwards. Natural gas trends upwards, overtaking coal as the leading emissions source in the middle 2010s. Hydrocarbon and Petroleum are both minor contributors to US CO2 emissions relative to Coal and Natural Gas - but they may exhibit slight seasonality as well.\n\n\n2) Lag Plots\n Examining lags in time series data highlights any potential correlations between steps along the time interval –> i.e. is energy production at time t correlated with energy production at time t-1? Does the previous year’s energy production impact energy production in future years (e.g. perhaps because of investments made in infrastructure or the passage of legislation that would affect production). Lag plots also help to understand any potential seasonality in the data.\n\n\n\n\n1 month, 6 month, 1 year, 5 year Lags in Coal CO2 Emissions\n\n\n\n\n\n\n\n\n1 month, 6 month, 1 year, 5 year Lags in Hydrocarbon Gas CO2 Emissions\n\n\n\n\n\n\n\n\n1 month, 6 month, 1 year, 5 year Lags in Petroleum Gas CO2 Emissions\n\n\n\n\n\n\n\n\n1 month, 6 month, 1 year, 5 year Lags in Natural Gas CO2 Emissions\n\n\n\n\n\n\nIn coal production, there’s a weak autocorrelation between the data and a 1 month lag, indicating that there is some connection between month to month production but other factors may play a larger role. Lags 6 and 60 are essentially random; thus, it’s likely that there isn’t a correlation between the time series data and 6 month effects or lags farther away. Lag 12 shows the strongest correlation - indicating that there’s a strong relationship in CO2 emissions from coal on a yearly basis.\nEmissions from hydrocarbon gas are strongly correlated with 1 month lag, 6 month lag, and 12 month lag. This indicates that there are strong seasonal effects year to year in emissions/usage of Natural Gas, as well as trends upwards over time in NatGas usage that lead to correlations month to month as well.\nPetroleum Gas exhibited very weak correlations across lags examined. Even the 1 month and 1 year lags were only weakly autocorrelated so there may be less seasonality at play with emissions here.\nFinally, Natural gas is HIGHLY correlated across lats. Lag 12 (1 year lag) shows a very strong relationship with the time series, with lag 60 (5 year lag) showing another strong relationship. On the other hand, lag 6 is almost randomly related to the time series, so seasonal effects are strong here.\n\n\n\n3) Decompose data\n\nCO2 Emissions from Coal\n\n\n\n\n\n\n\nCO2 Emissions from Petroleum\n\n\n\n\n\n\n\nCO2 Emissions from Hydrocarbon gas\n\n\n\n\n\n\n\nCO2 Emissions from Natural Gas\n\n\n\n\n\nThis is the first dataset to truly exhibit seasonal effects. CO2 emissions across all four fuel sources exhibit either multiplicative or additive seasonality, indicating that fuel usage and correlated CO2 emissions are highly periodic and tied to human behavior. In hot months, emissions increase and in cold months they tend to decrease. For 3/4 fuel sources, these seasonal swings are increasing over time, while for hydrocarbon gas these swings are constant over time.\n\n\n\n4) ACF/PACF Plots \nNote: Now, with a sense of how these different fuel sources impact CO2 emissions we will be model building and forecasting using Total CO2 Emissions. In a final review, I may dive into individual forecasts if necessary.\nAutocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots are useful for showing the correlation coefficients between a time series and its own lagged values. ACF looks at the time series and previous values of the series measured for different lag lengths - while PACF looks directly at the correlation between a time series and a given lag. Both are also useful for identifying if the data has stationarity - where the mean, variance, and autocorrelation structure do not change over time.\nBefore building ACF/PACF plots the data is logged and differenced - to examine seasonality and cyclicality directly. Differencing occurs by subtracting the previous observation from the current observation to remove temporal dependence.\n\n\n\n\n\n\n\n\nThe original data was highly non-stationary - and required logging the value of CO2 emissions, and one difference. Two differences would have overdifferenced and overcomplicated the data. Additionally, with one difference the ADF test below returns a p-value of 0.05. There are still correlated lags at 6,12,24 but those will be explored during model diagnostics. \n\n\n5) Augmented Dickey-Fuller Test\nADF is used to check that the data is stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  total$log_value %>% diff()\nDickey-Fuller = -18.926, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nThe Augmented Dickey-Fuller tests are used to assess how stationary the data is. An ADF test of all energy production outputs produced a p-value of 0.01. Thus, we can reject the null hypothesis of no stationarity at the 1% level - indicating that this series does in fact exhibit stationarity now. \n \nSource code for the above analysis: Github"
  },
  {
    "objectID": "arma_models.html#dataset-4---co2-emissions",
    "href": "arma_models.html#dataset-4---co2-emissions",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Dataset 4 - CO2 Emissions",
    "text": "Dataset 4 - CO2 Emissions\n\n\n\n\n1) Data Review\nFrom EDA, we looked at ACF graphs and checked ADF to see if the data was stationary. The data was not stationary so we take the log and difference to make it stationary.\nLog Transform coal, natural gas, petro, and hydrocarbon gas - and plot\n\n\n\n\n\nTaking the log of the data changed it significantly! An ADF test reveals the data is almost stationary. According to the ADF test, coal and petroleum require another differencing, while hydro and natural gas are already stationary. We can experiment with this during model building.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  coal_ts %>% diff()\nDickey-Fuller = -17.759, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  hydro_ts\nDickey-Fuller = -4.6126, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  natural_ts\nDickey-Fuller = -5.1735, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  petro_ts %>% diff()\nDickey-Fuller = -14.139, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\n\n\n2) ACF/PACF Plots\nACF/PACF plots are used to select potential p and q values for a SARIMA(p,d,q)(P,D,Q) model. SARIMA is used because the data contains seasonal components and had differencing. From these plots, some potential p,q,P,Q values are selected to fit a SARIMA model with. Separate models have to be fit for each energy source, thus separate ACF/PACF plots are shown below.\nCoal\n\n\n\n\n\nDifferencing: d = 0,1; D = 0,1  Moving average order: q = 0,1; Q = 0,1,2,3  Autoregressive terms: p = 0,1, P = 0,1 \nHydrocarbon\n\n\n\n\n\nDifferencing: d = 0,1; D = 0,1  Moving average order: q = 0,1,2,3,4; Q = 0,1,2,3  Autoregressive terms: p = 0,1, P = 0,1,2 \nNatural Gas\n\n\n\n\n\nDifferencing: d = 0,1; D = 0,1  Moving average order (ACF): q = 0,1,2; Q = 0,1,2  Autoregressive terms (PACF): p = 0,1,2,3; P = 0,1,2,3 \nPetroleum\n\n\n\n\n\nDifferencing: d = 0,1; D = 0,1  Moving average order: q = 0,1; Q = 0,1  Autoregressive terms: p = 0,1,2,3,4; P = 0,1,2 \n\n\n3) Fit SARIMA models and Model Diagnostics\nCoal\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-1713.934\n-1709.559\n-1713.928\n\n\n0\n1\n0\n1\n1\n0\n-1808.766\n-1800.016\n-1808.746\n\n\n0\n1\n0\n0\n1\n1\n-1933.114\n-1924.364\n-1933.094\n\n\n0\n1\n0\n1\n1\n1\n-1934.556\n-1921.431\n-1934.515\n\n\n0\n1\n0\n0\n1\n2\n-1935.279\n-1922.154\n-1935.238\n\n\n0\n1\n0\n1\n1\n2\n-1933.530\n-1916.030\n-1933.461\n\n\n0\n1\n1\n0\n1\n0\n-1756.164\n-1747.414\n-1756.144\n\n\n0\n1\n1\n1\n1\n0\n-1840.869\n-1827.744\n-1840.828\n\n\n0\n1\n1\n0\n1\n1\n-1954.295\n-1941.170\n-1954.254\n\n\n0\n1\n1\n1\n1\n1\n-1957.260\n-1939.760\n-1957.191\n\n\n0\n1\n1\n0\n1\n2\n-1958.498\n-1940.998\n-1958.429\n\n\n0\n1\n1\n1\n1\n2\n-1956.844\n-1934.969\n-1956.741\n\n\n1\n1\n0\n0\n1\n0\n-1749.433\n-1740.683\n-1749.412\n\n\n1\n1\n0\n1\n1\n0\n-1832.690\n-1819.565\n-1832.649\n\n\n1\n1\n0\n0\n1\n1\n-1946.043\n-1932.918\n-1946.002\n\n\n1\n1\n0\n1\n1\n1\n-1949.070\n-1931.570\n-1949.001\n\n\n1\n1\n0\n0\n1\n2\n-1950.206\n-1932.706\n-1950.138\n\n\n1\n1\n0\n1\n1\n2\n-1948.466\n-1926.591\n-1948.363\n\n\n1\n1\n1\n0\n1\n0\n-1755.139\n-1742.014\n-1755.097\n\n\n1\n1\n1\n1\n1\n0\n-1862.508\n-1845.008\n-1862.440\n\n\n1\n1\n1\n0\n1\n1\n-1983.036\n-1965.536\n-1982.967\n\n\n1\n1\n1\n1\n1\n1\n-1983.376\n-1961.500\n-1983.272\n\n\n1\n1\n1\n0\n1\n2\n-1984.070\n-1962.195\n-1983.967\n\n\n\n\n\nHydrocarbon\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-873.1263\n-868.7513\n-873.1194\n\n\n0\n1\n0\n1\n1\n0\n-1002.8832\n-994.1332\n-1002.8627\n\n\n0\n1\n0\n2\n1\n0\n-1111.3376\n-1098.2125\n-1111.2964\n\n\n0\n1\n0\n0\n1\n1\n-1203.8541\n-1195.1040\n-1203.8335\n\n\n0\n1\n0\n1\n1\n1\n-1201.9027\n-1188.7776\n-1201.8616\n\n\n0\n1\n0\n2\n1\n1\n-1204.4410\n-1186.9409\n-1204.3722\n\n\n0\n1\n0\n0\n1\n2\n-1201.9142\n-1188.7891\n-1201.8730\n\n\n0\n1\n0\n1\n1\n2\n-1201.4663\n-1183.9662\n-1201.3976\n\n\n0\n1\n0\n2\n1\n2\n-1203.6509\n-1181.7757\n-1203.5476\n\n\n0\n1\n0\n0\n1\n3\n-1203.5805\n-1186.0804\n-1203.5117\n\n\n0\n1\n0\n1\n1\n3\n-1202.5083\n-1180.6331\n-1202.4050\n\n\n0\n1\n1\n0\n1\n0\n-1060.3161\n-1051.5660\n-1060.2955\n\n\n0\n1\n1\n1\n1\n0\n-1186.1759\n-1173.0508\n-1186.1347\n\n\n0\n1\n1\n2\n1\n0\n-1287.0822\n-1269.5821\n-1287.0135\n\n\n0\n1\n1\n0\n1\n1\n-1347.1337\n-1334.0086\n-1347.0925\n\n\n0\n1\n1\n1\n1\n1\n-1345.9415\n-1328.4414\n-1345.8728\n\n\n0\n1\n1\n2\n1\n1\n-1346.4694\n-1324.5943\n-1346.3661\n\n\n0\n1\n1\n0\n1\n2\n-1346.0930\n-1328.5929\n-1346.0243\n\n\n0\n1\n1\n1\n1\n2\n-1346.8532\n-1324.9781\n-1346.7500\n\n\n0\n1\n1\n0\n1\n3\n-1346.6384\n-1324.7633\n-1346.5351\n\n\n0\n1\n2\n0\n1\n0\n-1061.4756\n-1048.3506\n-1061.4345\n\n\n0\n1\n2\n1\n1\n0\n-1189.6328\n-1172.1327\n-1189.5641\n\n\n0\n1\n2\n2\n1\n0\n-1295.2742\n-1273.3991\n-1295.1710\n\n\n0\n1\n2\n0\n1\n1\n-1358.8380\n-1341.3379\n-1358.7693\n\n\n0\n1\n2\n1\n1\n1\n-1357.1601\n-1335.2849\n-1357.0568\n\n\n0\n1\n2\n0\n1\n2\n-1357.2290\n-1335.3539\n-1357.1258\n\n\n0\n1\n3\n0\n1\n0\n-1061.0342\n-1043.5341\n-1060.9655\n\n\n0\n1\n3\n1\n1\n0\n-1188.4295\n-1166.5543\n-1188.3262\n\n\n0\n1\n3\n0\n1\n1\n-1357.7752\n-1335.9001\n-1357.6719\n\n\n0\n1\n4\n0\n1\n0\n-1080.6914\n-1058.8163\n-1080.5881\n\n\n1\n1\n0\n0\n1\n0\n-975.5913\n-966.8413\n-975.5708\n\n\n1\n1\n0\n1\n1\n0\n-1099.9589\n-1086.8339\n-1099.9178\n\n\n1\n1\n0\n2\n1\n0\n-1202.5387\n-1185.0386\n-1202.4699\n\n\n1\n1\n0\n0\n1\n1\n-1285.5188\n-1272.3937\n-1285.4776\n\n\n1\n1\n0\n1\n1\n1\n-1283.9443\n-1266.4442\n-1283.8755\n\n\n1\n1\n0\n2\n1\n1\n-1285.8820\n-1264.0069\n-1285.7787\n\n\n1\n1\n0\n0\n1\n2\n-1284.0408\n-1266.5407\n-1283.9721\n\n\n1\n1\n0\n1\n1\n2\n-1284.2978\n-1262.4227\n-1284.1946\n\n\n1\n1\n0\n0\n1\n3\n-1285.6719\n-1263.7968\n-1285.5687\n\n\n1\n1\n1\n0\n1\n0\n-1061.1029\n-1047.9778\n-1061.0618\n\n\n1\n1\n1\n1\n1\n0\n-1189.1817\n-1171.6816\n-1189.1130\n\n\n1\n1\n1\n2\n1\n0\n-1295.2879\n-1273.4128\n-1295.1846\n\n\n1\n1\n1\n0\n1\n1\n-1362.6442\n-1345.1441\n-1362.5755\n\n\n1\n1\n1\n1\n1\n1\n-1360.9720\n-1339.0969\n-1360.8687\n\n\n1\n1\n1\n0\n1\n2\n-1361.0473\n-1339.1722\n-1360.9440\n\n\n1\n1\n2\n0\n1\n0\n-1085.5112\n-1068.0111\n-1085.4425\n\n\n1\n1\n2\n1\n1\n0\n-1199.6741\n-1177.7990\n-1199.5708\n\n\n1\n1\n2\n0\n1\n1\n-1366.4262\n-1344.5511\n-1366.3230\n\n\n1\n1\n3\n0\n1\n0\n-1083.6212\n-1061.7461\n-1083.5179\n\n\n\n\n\nNatural Gas\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n-1542.895\n-1538.520\n-1542.889\n\n\n0\n1\n0\n1\n1\n0\n-1658.087\n-1649.337\n-1658.066\n\n\n0\n1\n0\n2\n1\n0\n-1725.448\n-1712.323\n-1725.407\n\n\n0\n1\n0\n3\n1\n0\n-1753.627\n-1736.127\n-1753.558\n\n\n0\n1\n0\n0\n1\n1\n-1792.413\n-1783.663\n-1792.392\n\n\n0\n1\n0\n1\n1\n1\n-1792.921\n-1779.796\n-1792.880\n\n\n0\n1\n0\n2\n1\n1\n-1792.694\n-1775.194\n-1792.625\n\n\n0\n1\n0\n3\n1\n1\n-1790.751\n-1768.876\n-1790.648\n\n\n0\n1\n0\n0\n1\n2\n-1793.287\n-1780.162\n-1793.246\n\n\n0\n1\n0\n1\n1\n2\n-1792.405\n-1774.904\n-1792.336\n\n\n0\n1\n0\n2\n1\n2\n-1790.737\n-1768.861\n-1790.633\n\n\n0\n1\n1\n0\n1\n0\n-1630.824\n-1622.074\n-1630.803\n\n\n0\n1\n1\n1\n1\n0\n-1734.020\n-1720.895\n-1733.979\n\n\n0\n1\n1\n2\n1\n0\n-1785.860\n-1768.359\n-1785.791\n\n\n0\n1\n1\n3\n1\n0\n-1810.176\n-1788.301\n-1810.073\n\n\n0\n1\n1\n0\n1\n1\n-1831.400\n-1818.275\n-1831.358\n\n\n0\n1\n1\n1\n1\n1\n-1832.111\n-1814.611\n-1832.043\n\n\n0\n1\n1\n2\n1\n1\n-1831.441\n-1809.566\n-1831.337\n\n\n0\n1\n1\n0\n1\n2\n-1832.460\n-1814.960\n-1832.391\n\n\n0\n1\n1\n1\n1\n2\n-1831.015\n-1809.140\n-1830.912\n\n\n0\n1\n2\n0\n1\n0\n-1669.981\n-1656.855\n-1669.939\n\n\n0\n1\n2\n1\n1\n0\n-1772.096\n-1754.595\n-1772.027\n\n\n0\n1\n2\n2\n1\n0\n-1830.306\n-1808.431\n-1830.203\n\n\n0\n1\n2\n0\n1\n1\n-1870.805\n-1853.305\n-1870.736\n\n\n0\n1\n2\n1\n1\n1\n-1872.027\n-1850.152\n-1871.924\n\n\n0\n1\n2\n0\n1\n2\n-1872.589\n-1850.714\n-1872.486\n\n\n1\n1\n0\n0\n1\n0\n-1582.644\n-1573.894\n-1582.623\n\n\n1\n1\n0\n1\n1\n0\n-1697.097\n-1683.972\n-1697.056\n\n\n1\n1\n0\n2\n1\n0\n-1756.762\n-1739.262\n-1756.693\n\n\n1\n1\n0\n3\n1\n0\n-1783.229\n-1761.354\n-1783.126\n\n\n1\n1\n0\n0\n1\n1\n-1814.581\n-1801.456\n-1814.540\n\n\n1\n1\n0\n1\n1\n1\n-1814.929\n-1797.428\n-1814.860\n\n\n1\n1\n0\n2\n1\n1\n-1814.363\n-1792.487\n-1814.259\n\n\n1\n1\n0\n0\n1\n2\n-1815.239\n-1797.739\n-1815.170\n\n\n1\n1\n0\n1\n1\n2\n-1814.007\n-1792.132\n-1813.904\n\n\n1\n1\n1\n0\n1\n0\n-1691.456\n-1678.331\n-1691.415\n\n\n1\n1\n1\n1\n1\n0\n-1787.334\n-1769.834\n-1787.265\n\n\n1\n1\n1\n2\n1\n0\n-1844.790\n-1822.915\n-1844.687\n\n\n1\n1\n1\n0\n1\n1\n-1892.790\n-1875.290\n-1892.721\n\n\n1\n1\n1\n1\n1\n1\n-1894.348\n-1872.473\n-1894.244\n\n\n1\n1\n1\n0\n1\n2\n-1894.850\n-1872.975\n-1894.747\n\n\n1\n1\n2\n0\n1\n0\n-1690.210\n-1672.710\n-1690.141\n\n\n1\n1\n2\n1\n1\n0\n-1786.554\n-1764.679\n-1786.451\n\n\n1\n1\n2\n0\n1\n1\n-1890.932\n-1869.057\n-1890.829\n\n\n2\n1\n0\n0\n1\n0\n-1632.125\n-1618.999\n-1632.083\n\n\n2\n1\n0\n1\n1\n0\n-1736.742\n-1719.242\n-1736.673\n\n\n2\n1\n0\n2\n1\n0\n-1790.699\n-1768.824\n-1790.595\n\n\n2\n1\n0\n0\n1\n1\n-1836.138\n-1818.638\n-1836.069\n\n\n2\n1\n0\n1\n1\n1\n-1837.311\n-1815.436\n-1837.208\n\n\n2\n1\n0\n0\n1\n2\n-1837.705\n-1815.830\n-1837.602\n\n\n2\n1\n1\n0\n1\n0\n-1689.826\n-1672.326\n-1689.758\n\n\n2\n1\n1\n1\n1\n0\n-1786.016\n-1764.141\n-1785.913\n\n\n2\n1\n1\n0\n1\n1\n-1890.921\n-1869.045\n-1890.817\n\n\n2\n1\n2\n0\n1\n0\n-1704.664\n-1682.789\n-1704.561\n\n\n3\n1\n0\n0\n1\n0\n-1640.016\n-1622.516\n-1639.948\n\n\n3\n1\n0\n1\n1\n0\n-1740.415\n-1718.540\n-1740.312\n\n\n3\n1\n0\n0\n1\n1\n-1839.331\n-1817.456\n-1839.228\n\n\n3\n1\n1\n0\n1\n0\n-1697.614\n-1675.739\n-1697.511\n\n\n\n\n\nPetroleum\n\n\n\n\n\np\nd\nq\nP\nD\nQ\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n0\n1\n0\n107.05453\n111.42955\n107.06137\n\n\n0\n1\n0\n1\n1\n0\n-84.86274\n-76.11269\n-84.84219\n\n\n0\n1\n0\n2\n1\n0\n-201.31645\n-188.19138\n-201.27529\n\n\n0\n1\n0\n0\n1\n1\n-266.32053\n-257.57048\n-266.29998\n\n\n0\n1\n0\n1\n1\n1\n-270.27867\n-257.15359\n-270.23750\n\n\n0\n1\n0\n2\n1\n1\n-269.47817\n-251.97808\n-269.40945\n\n\n0\n1\n1\n0\n1\n0\n-199.14932\n-190.39927\n-199.12878\n\n\n0\n1\n1\n1\n1\n0\n-403.85305\n-390.72798\n-403.81188\n\n\n0\n1\n1\n2\n1\n0\n-496.00647\n-478.50637\n-495.93775\n\n\n0\n1\n1\n0\n1\n1\n-572.47243\n-559.34735\n-572.43126\n\n\n0\n1\n1\n1\n1\n1\n-582.56629\n-565.06619\n-582.49756\n\n\n0\n1\n1\n2\n1\n1\n-581.20532\n-559.33020\n-581.10205\n\n\n1\n1\n0\n0\n1\n0\n-48.31077\n-39.56072\n-48.29022\n\n\n1\n1\n0\n1\n1\n0\n-251.91514\n-238.79007\n-251.87397\n\n\n1\n1\n0\n2\n1\n0\n-348.57302\n-331.07293\n-348.50430\n\n\n1\n1\n0\n0\n1\n1\n-424.90521\n-411.78013\n-424.86404\n\n\n1\n1\n0\n1\n1\n1\n-430.57355\n-413.07345\n-430.50482\n\n\n1\n1\n0\n2\n1\n1\n-429.03517\n-407.16005\n-428.93190\n\n\n1\n1\n1\n0\n1\n0\n-208.42018\n-195.29510\n-208.37901\n\n\n1\n1\n1\n1\n1\n0\n-403.12235\n-385.62225\n-403.05362\n\n\n1\n1\n1\n2\n1\n0\n-496.40992\n-474.53479\n-496.30665\n\n\n1\n1\n1\n0\n1\n1\n-571.40102\n-553.90092\n-571.33229\n\n\n1\n1\n1\n1\n1\n1\n-580.83795\n-558.96283\n-580.73468\n\n\n2\n1\n0\n0\n1\n0\n-95.61907\n-82.49400\n-95.57791\n\n\n2\n1\n0\n1\n1\n0\n-316.82671\n-299.32661\n-316.75798\n\n\n2\n1\n0\n2\n1\n0\n-412.05095\n-390.17583\n-411.94768\n\n\n2\n1\n0\n0\n1\n1\n-486.30003\n-468.79993\n-486.23130\n\n\n2\n1\n0\n1\n1\n1\n-499.14862\n-477.27350\n-499.04535\n\n\n2\n1\n1\n0\n1\n0\n-212.31668\n-194.81658\n-212.24795\n\n\n2\n1\n1\n1\n1\n0\n-406.03542\n-384.16029\n-405.93215\n\n\n2\n1\n1\n0\n1\n1\n-570.32544\n-548.45032\n-570.22217\n\n\n3\n1\n0\n0\n1\n0\n-114.39490\n-96.89480\n-114.32617\n\n\n3\n1\n0\n1\n1\n0\n-338.03855\n-316.16343\n-337.93528\n\n\n3\n1\n0\n0\n1\n1\n-507.26295\n-485.38782\n-507.15968\n\n\n3\n1\n1\n0\n1\n0\n-211.01216\n-189.13704\n-210.90889\n\n\n4\n1\n0\n0\n1\n0\n-127.23810\n-105.36298\n-127.13483\n\n\n\n\n\n\n\n4) Model Diagnostics\nWith the above exploration, a best fit model is chosen for each energy source by minimizing AIC.\nCoal\nThe SARIMA model that minimizes AIC and AICc is SARIMA(1,1,1)(0,1,2)\n\n\n\n\n\nSeries: coal_ts \nARIMA(1,1,1)(0,1,2)[12] \n\nCoefficients:\n         ar1      ma1     sma1     sma2\n      0.7153  -0.9330  -0.7101  -0.0771\ns.e.  0.0468   0.0262   0.0463   0.0433\n\nsigma^2 = 0.001933:  log likelihood = 997.03\nAIC=-1984.07   AICc=-1983.97   BIC=-1962.19\n\nTraining set error measures:\n                      ME       RMSE        MAE         MPE      MAPE      MASE\nTraining set -0.00220553 0.04333907 0.02966952 -0.05188677 0.6181671 0.5017093\n                    ACF1\nTraining set -0.02887885\n\n\nHydrocarbon\nThe SARIMA model that minimizes AIC and AICc is SARIMA(1,1,2)(0,1,1)\n\n\n\n\n\nSeries: hydro_ts \nARIMA(1,1,2)(0,1,1)[12] \n\nCoefficients:\n         ar1      ma1     ma2     sma1\n      0.6838  -1.2657  0.3178  -0.8718\ns.e.  0.1003   0.1177  0.0981   0.0302\n\nsigma^2 = 0.005473:  log likelihood = 688.21\nAIC=-1366.43   AICc=-1366.32   BIC=-1344.55\n\nTraining set error measures:\n                      ME       RMSE        MAE        MPE     MAPE      MASE\nTraining set 0.003801311 0.07292737 0.05689759 0.09963854 2.964171 0.6705054\n                    ACF1\nTraining set 0.002831291\n\n\nNatural Gas\nThe SARIMA model that minimizes AIC and AICc is SARIMA(1,1,1)(0,1,2)\n\n\n\n\n\nSeries: natural_ts \nARIMA(1,1,1)(0,1,2)[12] \n\nCoefficients:\n         ar1      ma1     sma1     sma2\n      0.5435  -0.9253  -0.6784  -0.0909\ns.e.  0.0438   0.0183   0.0439   0.0449\n\nsigma^2 = 0.002253:  log likelihood = 952.42\nAIC=-1894.85   AICc=-1894.75   BIC=-1872.97\n\nTraining set error measures:\n                      ME       RMSE        MAE        MPE      MAPE     MASE\nTraining set 0.002460763 0.04678813 0.03593403 0.04625378 0.7828864 0.655979\n                    ACF1\nTraining set 0.004392229\n\n\nPetroleum\nThe SARIMA model that minimizes AIC, BIC, and AICc is SARIMA(0,1,1)(1,1,1)\n\n\n\n\n\nSeries: petro_ts \nARIMA(0,1,1)(1,1,1)[12] \n\nCoefficients:\n          ma1     sar1     sma1\n      -0.8639  -0.1659  -0.8360\ns.e.   0.0226   0.0469   0.0289\n\nsigma^2 = 0.02081:  log likelihood = 295.28\nAIC=-582.57   AICc=-582.5   BIC=-565.07\n\nTraining set error measures:\n                       ME     RMSE       MAE        MPE     MAPE      MASE\nTraining set -0.001821531 0.142328 0.1073413 -0.7744743 6.447119 0.6979237\n                   ACF1\nTraining set 0.01685106\n\n\n\n\n5) Fit a SARIMA(p,d,q)(P,D,Q) model using auto.arima()\nCoal\n\n\nSeries: coal_ts \nARIMA(1,1,1)(0,1,2)[12] \n\nCoefficients:\n         ar1      ma1     sma1     sma2\n      0.7153  -0.9330  -0.7101  -0.0771\ns.e.  0.0468   0.0262   0.0463   0.0433\n\nsigma^2 = 0.001933:  log likelihood = 997.03\nAIC=-1984.07   AICc=-1983.97   BIC=-1962.19\n\n\nThe model chosen was ARIMA(1,1,1)(0,1,2) which is the same as I chose via manual fitting!\nHydrocarbon\n\n\nSeries: hydro_ts \nARIMA(2,0,1)(2,1,2)[12] with drift \n\nCoefficients:\n         ar1      ar2      ma1     sar1     sar2     sma1     sma2  drift\n      1.2627  -0.2778  -0.8241  -0.4107  -0.1018  -0.4327  -0.3460  6e-04\ns.e.  0.0810   0.0733   0.0587   0.2109   0.0598   0.2094   0.1941  5e-04\n\nsigma^2 = 0.005468:  log likelihood = 693.26\nAIC=-1368.51   AICc=-1368.2   BIC=-1329.12\n\n\nThe model chosen was ARIMA(2,0,1)(0,1,2) which is different from my chosen model of ARIMA(1,1,2)(0,1,1).\nNatural Gas\n\n\nSeries: natural_ts \nARIMA(2,1,1)(1,1,1)[12] \n\nCoefficients:\n         ar1      ar2      ma1    sar1     sma1\n      0.5500  -0.0177  -0.9225  0.1084  -0.7961\ns.e.  0.0468   0.0442   0.0201  0.0573   0.0396\n\nsigma^2 = 0.002258:  log likelihood = 952.25\nAIC=-1892.51   AICc=-1892.36   BIC=-1866.26\n\n\nThe model chosen was ARIMA(2,1,1)(1,1,1) which was different from my chosen model of ARIMA(1,1,1)(0,1,2).\nPetroleum\n\n\nSeries: petro_ts \nARIMA(0,1,1)(0,0,2)[12] \n\nCoefficients:\n          ma1    sma1    sma2\n      -0.8842  0.0798  0.1886\ns.e.   0.0214  0.0443  0.0378\n\nsigma^2 = 0.0244:  log likelihood = 262.41\nAIC=-516.83   AICc=-516.76   BIC=-499.24\n\n\nThe model chosen was ARIMA(0,1,1)(0,0,2) which was slightly different from my chosen model of ARIMA(0,1,1)(1,1,1). The seasonal effects were handled differently by auto.arima()\n\n\n6) Forecast\nForecasting CO2 Emissions by source for the next 5 years\nCoal\n\n\n\n\n\nHydrocarbon\n\n\n\n\n\nNatural Gas\n\n\n\n\n\nPetroleum\n\n\n\n\n\nAccording to our models, coal emissions are expected to continue their steep downwards trend over the next 5 years, while CO2 Emissions from Petroleum are also expected to decrease over the next 5 years - albeit slightly less dramatically. CO2 emissions from both Hydrocarbon Gas and Natural Gas are forecasted to increase over the next 5 years.\n\n\n7) Benchmark\nCompare the models to baseline benchmark methods to ensure they’re performing above.\nCoal\n\n\n\n\n\nAll benchmark methods predict CO2 emissions to stay steady or increase over the next 5 years. It looks like our model outperforms the benchmark methods.\nHydrocarbon\n\n\n\n\n\nBoth fit and seasonal naive methods predict an upward trend in CO2 emissions, with seasonal variation preserved. Drift, mean, and naive methods predict constant emissions and our model is far better.\nNatural Gas\n\n\n\n\n\nDrift, Mean and Naive methods predict constant emissions over the next 5 years. Seasonal naive predicts increasing CO2 emissions with seasonal variation, but the fit outperforms seasonal naive and it follows the trend slightly better.\nPetroleum\n\n\n\n\n\nThe fit and seasonal naive preserve the seasonal variation but the model fit forecasts decreasing CO2 emissions (following the trend) and seasonal naive predicts increasing seasonal variation. Drift, mean, and naive benchmark methods again severely underperform.\n\n\n8) Seasonal Cross Validation\n1 Step Ahead\nCoal ARIMA(1,1,1)(0,1,2) for both auto.arima() and manual fitting.\n\n\n [1] 0.04619373 0.04570981 0.04725047 0.04745566 0.04854170 0.04904679\n [7] 0.04993501 0.05057058 0.05137274 0.05206507 0.05282980 0.05354680\n[13] 0.05429527 0.05502300 0.05576439 0.05649678 0.05723510 0.05796952\n[19] 0.05870651 0.05944180 0.06017821 0.06091388 0.06165004 0.06238588\n[25] 0.06312193 0.06430790 0.06632118 0.06833454 0.07034785 0.07236119\n[31] 0.07437451 0.07638784 0.07840117 0.08041450 0.08242783 0.08444115\n[37] 0.08645448 0.08846781 0.09048114 0.09249447 0.09450779 0.09652112\n[43] 0.09853445 0.10054778 0.10256111 0.10457443 0.10658776 0.10860109\n[49] 0.11061442 0.11262775 0.11464107 0.11665440 0.11866773 0.12068106\n[55] 0.12269439 0.12470771 0.12672104 0.12873437 0.13074770 0.13276103\n\n\nHydrocarbon ARIMA(2,0,1)(0,1,2) for auto.arima() and ARIMA(1,1,2)(0,1,1) from manual fitting.\n\n\n [1] 0.07974090 0.21231107 0.34844198 0.43904916 0.46729056 0.43355642\n [7] 0.35332484 0.25158786 0.15561699 0.08803235 0.06183177 0.07836086\n[13] 0.12837006 0.19553350 0.26127758 0.30958432 0.33059942 0.32230591\n[19] 0.29008470 0.24452063 0.19819837 0.16239099 0.14446263 0.14653472\n[25] 0.16559137 0.19482700 0.22575927 0.25050036 0.26361288 0.26314655\n[31] 0.25069987 0.23060751 0.20855614 0.19003452 0.17901631 0.17716960\n[37] 0.18372372 0.19594748 0.21004964 0.22223107 0.22961376 0.23083438\n[43] 0.22619708 0.21740091 0.20695879 0.19748680 0.19105279 0.18873644\n[49] 0.19048299 0.19525160 0.20138694 0.20709667 0.21090569 0.21197878\n[55] 0.21024853 0.20634046 0.20133780 0.19646233 0.19275939 0.19086346\n\n\n [1] 0.08971719 0.09637873 0.09879890 0.09967010 0.09997514 0.10007317\n [7] 0.10009549 0.10009014 0.10007468 0.10005553 0.10003503 0.10001403\n[13] 0.09999286 0.09997162 0.09995036 0.09992909 0.09990782 0.09988655\n[19] 0.09986527 0.09984400 0.09982272 0.09980145 0.09978017 0.09975890\n[25] 0.09973762 0.09971635 0.09969508 0.09967380 0.09965253 0.09963125\n[31] 0.09960998 0.09958870 0.09956743 0.09954615 0.09952488 0.09950361\n[37] 0.09948233 0.09946106 0.09943978 0.09941851 0.09939723 0.09937596\n[43] 0.09935468 0.09933341 0.09931213 0.09929086 0.09926959 0.09924831\n[49] 0.09922704 0.09920576 0.09918449 0.09916321 0.09914194 0.09912066\n[55] 0.09909939 0.09907812 0.09905684 0.09903557 0.09901429 0.09899302\n\n\n\n\n\nFrom this, it appears that ARIMA(2,0,1)(0,1,2) has much more variable RMSE, but overall is a better fit model\nNatural gas ARIMA(2,1,1)(1,1,1) for auto.arima() and ARIMA(1,1,1)(0,1,2) from manual fitting.\n\n\n [1] 0.05279172 0.17922373 0.31912281 0.42672446 0.48067602 0.48246156\n [7] 0.44856163 0.40070412 0.35771745 0.33088375 0.32303682 0.33045894\n[13] 0.34610679 0.36277882 0.37530316 0.38141445 0.38148409 0.37755569\n[19] 0.37219683 0.36756755 0.36491146 0.36448378 0.36579586 0.36800119\n[25] 0.37026257 0.37199741 0.37296817 0.37324094 0.37306752 0.37275048\n[31] 0.37253697 0.37256333 0.37285056 0.37333544 0.37391662 0.37449699\n[37] 0.37501116 0.37543457 0.37577755 0.37607094 0.37635031 0.37664385\n[43] 0.37696642 0.37731945 0.37769487 0.37808058 0.37846534 0.37884186\n[49] 0.37920762 0.37956414 0.37991518 0.38026490 0.38061650 0.38097161\n[55] 0.38133027 0.38169145 0.38205372 0.38241579 0.38277683 0.38313660\n\n\n [1] 0.05169864 0.08091653 0.09517965 0.10221910 0.10576896 0.10763298\n [7] 0.10868248 0.10933841 0.10980416 0.11017801 0.11050744 0.11081540\n[13] 0.11111299 0.11140556 0.11169571 0.11198468 0.11227308 0.11256121\n[19] 0.11284921 0.11313715 0.11342505 0.11371294 0.11400082 0.11428870\n[25] 0.11457658 0.11486445 0.11515233 0.11544020 0.11572808 0.11601595\n[31] 0.11630383 0.11659170 0.11687958 0.11716745 0.11745533 0.11774320\n[37] 0.11803108 0.11831895 0.11860683 0.11889470 0.11918258 0.11947045\n[43] 0.11975833 0.12004620 0.12033408 0.12062195 0.12090983 0.12119770\n[49] 0.12148558 0.12177345 0.12206133 0.12234920 0.12263708 0.12292495\n[55] 0.12321283 0.12350070 0.12378858 0.12407645 0.12436433 0.12465220\n\n\n\n\n\nFrom this analysis, it looks like ARIMA(2,1,1)(1,1,1) is a better fit model.\nPetroleum ARIMA(0,1,1)(0,0,2) for auto.arima() and ARIMA(0,1,1)(1,1,1) from manual fitting.\n\n\n [1] 0.08598773 0.08658462 0.08718152 0.08777841 0.08837531 0.08897220\n [7] 0.08956909 0.09016599 0.09076288 0.09135978 0.09195667 0.09255356\n[13] 0.09315046 0.09374735 0.09434425 0.09494114 0.09553803 0.09613493\n[19] 0.09673182 0.09732872 0.09792561 0.09852250 0.09911940 0.09971629\n[25] 0.10031319 0.10091008 0.10150697 0.10210387 0.10270076 0.10329766\n[31] 0.10389455 0.10449144 0.10508834 0.10568523 0.10628213 0.10687902\n[37] 0.10747591 0.10807281 0.10866970 0.10926660 0.10986349 0.11046038\n[43] 0.11105728 0.11165417 0.11225107 0.11284796 0.11344485 0.11404175\n[49] 0.11463864 0.11523554 0.11583243 0.11642932 0.11702622 0.11762311\n[55] 0.11822001 0.11881690 0.12041907 0.12202312 0.12362717 0.12523122\n\n\n [1] 0.08598773 0.08658462 0.08718152 0.08777841 0.08837531 0.08897220\n [7] 0.08956909 0.09016599 0.09076288 0.09135978 0.09195667 0.09255356\n[13] 0.09315046 0.09374735 0.09434425 0.09494114 0.09553803 0.09613493\n[19] 0.09673182 0.09732872 0.09792561 0.09852250 0.09911940 0.09971629\n[25] 0.10031319 0.10091008 0.10150697 0.10210387 0.10270076 0.10329766\n[31] 0.10389455 0.10449144 0.10508834 0.10568523 0.10628213 0.10687902\n[37] 0.10747591 0.10807281 0.10866970 0.10926660 0.10986349 0.11046038\n[43] 0.11105728 0.11165417 0.11225107 0.11284796 0.11344485 0.11404175\n[49] 0.11463864 0.11523554 0.11583243 0.11642932 0.11702622 0.11762311\n[55] 0.11822001 0.11881690 0.12041907 0.12202312 0.12362717 0.12523122\n\n\n\n\n\nFrom this analysis, it looks like both models perform similarly and have the same RMSE.\n12 steps ahead forecast\nCoal ARIMA(1,1,1)(0,1,2) for both auto.arima() and manual fitting.\n\n\n[1] 525\n\n\n[1] 0.3327111\n\n\nHydrocarbon ARIMA(2,0,1)(0,1,2) for auto.arima() and ARIMA(1,1,2)(0,1,1) from manual fitting.\n\n\n[1] 525\n\n\n[1] 0.4455012\n\n\n[1] 0.4738333\n\n\nFrom this analysis, ARIMA(2,0,1)(0,1,2) from auto.arima() has a much lower RMSE.\nNatural gas ARIMA(2,1,1)(1,1,1) for auto.arima() and ARIMA(1,1,1)(0,1,2) from manual fitting.\n\n\n[1] 525\n\n\n[1] 0.4328701\n\n\n[1] 0.4238351\n\n\nFrom this analysis, ARIMA(1,1,1)(0,1,2) from manual fitting has a much lower RMSE and is a better fit model for this data.\nPetroleum ARIMA(0,1,1)(0,0,2) for auto.arima() and ARIMA(0,1,1)(1,1,1) from manual fitting.\n\n\n[1] 525\n\n\n[1] 0.367206\n\n\n[1] 0.367206\n\n\nFrom this analysis, it appears that both models perform the same and have the same RMSE. Either is a valid model to work with.\n\nSource code for the above analysis: Github"
  },
  {
    "objectID": "var.html#predicting-annual-co2-emissions-from-energy-cost-and-production-1990-2010",
    "href": "var.html#predicting-annual-co2-emissions-from-energy-cost-and-production-1990-2010",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Predicting annual CO2 Emissions from energy cost and production 1990-2010",
    "text": "Predicting annual CO2 Emissions from energy cost and production 1990-2010\nARIMAX Model\nData:\n\n\n        YEAR Total.Output Total.Energy Total.CO2\n1 1990-01-01   6021136938         8.25  5037.903\n2 1991-01-01   6084119403         8.21  4992.691\n3 1992-01-01   6098152107         8.13  5093.886\n4 1993-01-01   6324830225         8.25  5185.525\n5 1994-01-01   6422815640         8.30  5262.667\n6 1995-01-01   6630217247         8.28  5324.315\n\n\nPlotting the data\n\n\n\n\n\n\nBackground Literature\nThis paper, Forecasting of transportation-related energy demand and CO2 emissions in Turkey with different ML algorithms utilizes energy production as a key predictive variable for CO2 emissions in Turkey between 1990-2014. Though many authors use Energy Consumption as a predictor for CO2 Emissions, production and consumption are closely tied due to economic demand for energy, so production is a worthy proxy.\nThis paper, Machine Learning in Estimating CO2 Emissions from Electricity Generation uses ML techniques to produce accurate modeling of energy costs, and then using a cost-effective analysis to estimate CO2 emissions. As prices fluctuate around different fuel sources, and natural gas in the US takes off and coal usage diminishes, it will be interesting to see if these cost fluctuations have any impact on CO2 emissions.\n\n\ni) Choose response variables\nThe key response variable here is total CO2 emissions from 1990-2010. The independent variables are total electricity production and total cost of electricity between 1990-2010. Due to data availability, these years were chosen and analysis has to be done at the annual level.\n\n\nii) Log Transform\nLog transform cost of energy, energy production, and co2 emissions.\n\n\n\n\n\n\n\niii) Fit using auto.arima()\n\n\nSeries: dd.ts[, \"emissions_log\"] \nRegression with ARIMA(0,0,1) errors \n\nCoefficients:\n         ma1  intercept     Cost  Production\n      0.7030    -6.8854  -0.0696      0.6908\ns.e.  0.1988     2.1884   0.0313      0.0992\n\nsigma^2 = 0.0003311:  log likelihood = 56.22\nAIC=-102.43   AICc=-98.43   BIC=-97.21\n\nTraining set error measures:\n                        ME       RMSE        MAE          MPE      MAPE\nTraining set -0.0003141545 0.01637279 0.01201262 -0.004128035 0.1393188\n                  MASE       ACF1\nTraining set 0.5988199 0.04984199\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from Regression with ARIMA(0,0,1) errors\nQ* = 0.56437, df = 3, p-value = 0.9045\n\nModel df: 1.   Total lags used: 4\n\n\nFrom auto.arima(), our model is an ARIMAX. This is a Regression model with ARIMA(0,0,1) with no errors. Though the data is highly non-stationary, the Arima model is being fit on the residuals, not the data itself. To compare, in part iv, I fit a model manually.\n\n\niv) Fit the model manually\nFirst, fit the linear regression model predicting log_emissions using production, and cost of fuel. Then for the residuals, fit an ARIMA model.\n\n\n\nCall:\nlm(formula = emissions_log ~ cost_log + production_log, data = df)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.052447 -0.011610  0.008236  0.012098  0.023289 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    -9.02651    1.84466  -4.893 0.000117 ***\ncost_log       -0.10326    0.02634  -3.920 0.001003 ** \nproduction_log  0.78870    0.08362   9.432 2.18e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02116 on 18 degrees of freedom\nMultiple R-squared:  0.8915,    Adjusted R-squared:  0.8794 \nF-statistic: 73.92 on 2 and 18 DF,  p-value: 2.091e-09\n\n\nNow look at the residuals.\n\n\n\n\n\nInterestingly, from the ACF/PACF plots the data looks stationary without differencing. Let’s do some model fitting of different parameters to experiment. Because there aren’t statistically significantly correlated lags (via spikes), I will experiment with:\np = 0,1,2 q = 0,1,2 d = 0,1,2\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n-96.49384\n-95.49811\n-96.27162\n\n\n0\n2\n0\n-75.45593\n-74.51149\n-75.22063\n\n\n0\n1\n1\n-95.16930\n-93.17783\n-94.46342\n\n\n0\n2\n1\n-85.88958\n-84.00070\n-85.13958\n\n\n0\n1\n2\n-96.89392\n-93.90672\n-95.39392\n\n\n0\n2\n2\n-84.23001\n-81.39669\n-82.63001\n\n\n1\n1\n0\n-94.85582\n-92.86436\n-94.14994\n\n\n1\n2\n0\n-78.09931\n-76.21043\n-77.34931\n\n\n1\n1\n1\n-95.19374\n-92.20654\n-93.69374\n\n\n1\n2\n1\n-84.09131\n-81.25800\n-82.49131\n\n\n1\n1\n2\n-94.89881\n-90.91588\n-92.23215\n\n\n1\n2\n2\n-82.51812\n-78.74036\n-79.66098\n\n\n2\n1\n0\n-95.54231\n-92.55511\n-94.04231\n\n\n2\n2\n0\n-82.50156\n-79.66824\n-80.90156\n\n\n2\n1\n1\n-93.54351\n-89.56058\n-90.87684\n\n\n2\n2\n1\n-84.04287\n-80.26512\n-81.18573\n\n\n\n\n\nChoose the model that minmizes AIC, BIC, and AICc\n\n\n  p d q       AIC       BIC      AICc\n5 0 1 2 -96.89392 -93.90672 -95.39392\n\n\n  p d q       AIC       BIC      AICc\n1 0 1 0 -96.49384 -95.49811 -96.27162\n\n\n  p d q       AIC       BIC      AICc\n1 0 1 0 -96.49384 -95.49811 -96.27162\n\n\nThe model that minimizes AIC is ARIMA(0,1,2) and the model that minimizes AICc and BIC is ARIMA(0,1,0)\n\n\nv) Cross validation\nHere I use cross validation to find the best model from the above 3 models.\nauto.arima() chose Arima(0,0,1) manually - Arima(0,1,0) or Arima(0,1,2)\n\n\n\nRMSEs from the three models\n\n\n\n\n\nMean RMSEs across three models\n\n\n[1] 0.01723954 0.02011468 0.02088948 0.02166427\n\n\n[1] 0.02198930 0.02535139 0.02871347 0.03207556\n\n\n[1] 0.02015055 0.02034023 0.02098648 0.02163274\n\n\nThe best model via RMSEs is Arima(0,0,1) actually! By a small margin.\n\n\nvii) Forecast using model\n\n\nSeries: df$emissions_log \nRegression with ARIMA(0,0,1) errors \n\nCoefficients:\n         ma1  intercept     Cost  Production\n      0.7030    -6.8854  -0.0696      0.6908\ns.e.  0.1988     2.1884   0.0313      0.0992\n\nsigma^2 = 0.0003311:  log likelihood = 56.22\nAIC=-102.43   AICc=-98.43   BIC=-97.21\n\nTraining set error measures:\n                        ME       RMSE        MAE          MPE      MAPE\nTraining set -0.0003141545 0.01637279 0.01201262 -0.004128035 0.1393188\n                  MASE       ACF1\nTraining set 0.5988199 0.04984199\n\n\nThe model equation is: \\[\n\\theta(B) = 1 - 0.7939(B)\n\\]\n\n\n\n\n\nThe data only went until 2010, so emissions are forecasted for the next 10 years. The forecast from regression with ARIMA(0,0,1) predicts emissions steadily increase. Despite a dip in emissions in 2008, the forecast follows the upwards trend occurring at the end of the time series in 2010, which predicts growing emissions.\nSource code for the above analysis: Github"
  },
  {
    "objectID": "var.html#dataset-2---cost-of-energy",
    "href": "var.html#dataset-2---cost-of-energy",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Dataset 2 - Cost of Energy",
    "text": "Dataset 2 - Cost of Energy"
  },
  {
    "objectID": "var.html#dataset-3---co2-emissions",
    "href": "var.html#dataset-3---co2-emissions",
    "title": "ARIMAX/SARIMAX/VAR",
    "section": "Dataset 3 - CO2 Emissions",
    "text": "Dataset 3 - CO2 Emissions"
  },
  {
    "objectID": "financial.html",
    "href": "financial.html",
    "title": "Financial Time Series Models",
    "section": "",
    "text": "Here, I use stock market data of the top fossil fuel company: ExxonMobil ($466.94 billion) and the top renewable energy stock: Nextera Energy ($167 billion) to model the financial data and the volatility of future returns using ARCH/GARCH models.\nData: Stock tickers = XOM and NEE\nCandlestick plot of EXXON Prices over the past years, as well as the returns.\nObservations: The XOM candlestick plot shows that XOM stock was between 80 and 100 USD per share between 2012 and 2018, with the exceptions of a few dips in 2016. However, the stock price has seen far more volatility in recent years, especially around 2020 and the beginning of the COVID-19 pandemic. In March 2020 the stock price plummeted to 32 USD per share, making a brief revival in June 2020 back to 50 USD per share, before dropping back to 31 USD per share in November 2020. The returns plot mirrors these findings, indicating that 2019 - 2021 saw a period of high volatility to be examined. The data appears non-stationary.\nCandlestick plot of NEE Prices over the past years, as well as the returns.\nObservations: The NEE candlestick plot shows that NEE stock has steadily risen since 2012. There are some periods of volatility and stock price drops between 2020 and 2022, as the stock dropped from 67 USD to 46 USD in March 2020 and 92 USD to 73 USD between December 2021 and February 2022. The data is highly non-stationary and will need to be differenced before modeling."
  },
  {
    "objectID": "financial.html#fit-model-for-returns-data",
    "href": "financial.html#fit-model-for-returns-data",
    "title": "Financial Time Series Models",
    "section": "Fit Model for Returns data",
    "text": "Fit Model for Returns data\n\nAR+ARCH/GARCH\nARIMA+ARCH/GARCH\n\n\n1) Exxon Data\n\nACF/PACF plots of the returns squared to determine if there are any remaining dependence structures not captured by ARCH/GARCH models.\n\n\n\n\n\n\n\n\nThe ACF/PACF plot of returns(^2) help determine if ARCH is sufficient, or whether an AR/ARIMA model needs to be fit. From the ACF plot, I can observe that lags 0-3.0 are correlated with the time series. From the PACF plot I can observe that lags 0-3.0 are again correlated with the time series. Because there are still significant lags on both plots, an ARIMA model should be fitted first and an ARCH model can be fitted to the residuals of the ARIMA model.\n\n\nFit ARIMA model first\n\na) Choose p,q,d values\n\n\nb) Model diagnostics\n\n\n\nCalculate residuals of ARIMA model\n\n\nFit ARCH models and do model diagnostics"
  },
  {
    "objectID": "financial.html#model-equation",
    "href": "financial.html#model-equation",
    "title": "Financial Time Series Models",
    "section": "Model Equation",
    "text": "Model Equation"
  },
  {
    "objectID": "financial.html#fit-model-for-returns-data-for-exxon-and-nextera-energy",
    "href": "financial.html#fit-model-for-returns-data-for-exxon-and-nextera-energy",
    "title": "Financial Time Series Models",
    "section": "Fit Model for Returns data for Exxon and Nextera Energy",
    "text": "Fit Model for Returns data for Exxon and Nextera Energy\n\nAR+ARCH/GARCH\nARIMA+ARCH/GARCH\n\n\n1) Exxon Data\n\nACF/PACF plots of the returns squared to determine if there are any remaining dependence structures not captured by ARCH/GARCH models.\n\n\n\n\n\n\n\n\nThe ACF/PACF plot of returns(^2) help determine if ARCH is sufficient, or whether an AR/ARIMA model needs to be fit. From the ACF plot, I can observe that lags 0-3.0 are correlated with the time series. From the PACF plot I can observe that lags 0-3.0 are again correlated with the time series. Because there are still significant lags on both plots, an ARIMA model should be fitted first and an ARCH model can be fitted to the residuals of the ARIMA model.\n\n\nFit ARIMA model first to original data\n\na) Check if original data is stationary\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts.exxon\nDickey-Fuller = 0.055138, Lag order = 13, p-value = 0.99\nalternative hypothesis: stationary\n\n\nExxon data is certainly not stationary, as seen from the ACF/PACF plots and ADF test shown above. Thus, to make the data stationary the log of Exxon Adjusted Closing Price is taken, and differencing is undertaken.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log.exxon %>% diff()\nDickey-Fuller = -13.168, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\nJust taking the log of EXXON stock data is not enough…the p-value of the ADF test is still 0.94, indicating that the data remains non-stationary. After differencing the data once, stationarity is achieved.\n\n\nb) Choose p,q,d values\nFrom the above ACF/PACF plots, values of p and q are chosen to investigate. Moving average order (found from ACF): q - 1,2,3,5 Autoregressive term (found from pACF): p - 1,2,3,5\n\n\nc) Model diagnostics\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n-13661.97\n-13650.27\n-13661.96\n\n\n0\n1\n1\n-13661.75\n-13644.20\n-13661.74\n\n\n0\n1\n2\n-13660.77\n-13637.39\n-13660.76\n\n\n0\n1\n3\n-13665.54\n-13636.30\n-13665.52\n\n\n0\n1\n4\n-13664.25\n-13629.17\n-13664.22\n\n\n0\n1\n5\n-13667.93\n-13627.01\n-13667.89\n\n\n1\n1\n0\n-13661.82\n-13644.28\n-13661.81\n\n\n1\n1\n1\n-13661.72\n-13638.33\n-13661.71\n\n\n1\n1\n2\n-13660.80\n-13631.57\n-13660.78\n\n\n1\n1\n3\n-13663.76\n-13628.68\n-13663.73\n\n\n1\n1\n4\n-13665.61\n-13624.68\n-13665.57\n\n\n1\n1\n5\n-13675.79\n-13629.02\n-13675.74\n\n\n2\n1\n0\n-13661.05\n-13637.66\n-13661.04\n\n\n2\n1\n1\n-13660.61\n-13631.37\n-13660.58\n\n\n2\n1\n2\n-13657.89\n-13622.81\n-13657.85\n\n\n2\n1\n3\n-13666.77\n-13625.85\n-13666.73\n\n\n2\n1\n4\n-13680.62\n-13633.85\n-13680.57\n\n\n2\n1\n5\n-13672.81\n-13620.19\n-13672.74\n\n\n3\n1\n0\n-13664.71\n-13635.48\n-13664.69\n\n\n3\n1\n1\n-13662.70\n-13627.62\n-13662.67\n\n\n3\n1\n2\n-13663.72\n-13622.79\n-13663.68\n\n\n3\n1\n3\n-13680.85\n-13634.07\n-13680.79\n\n\n3\n1\n4\n-13681.15\n-13628.53\n-13681.08\n\n\n4\n1\n0\n-13662.73\n-13627.65\n-13662.70\n\n\n4\n1\n1\n-13660.74\n-13619.81\n-13660.70\n\n\n4\n1\n2\n-13680.84\n-13634.06\n-13680.78\n\n\n4\n1\n3\n-13681.01\n-13628.38\n-13680.94\n\n\n5\n1\n0\n-13670.31\n-13629.38\n-13670.27\n\n\n5\n1\n1\n-13678.28\n-13631.51\n-13678.23\n\n\n5\n1\n2\n-13685.31\n-13632.69\n-13685.24\n\n\n\n\n\nMinimum AIC,BIC,and AICc\nThus, the ARIMA model that minimizes AIC and AICc is ARIMA(2,1,4).\n\n\n\nCalculate residuals of ARIMA model\n\n\n\nCall:\narima(x = log.exxon, order = c(2, 1, 4))\n\nCoefficients:\n         ar1      ar2      ma1     ma2      ma3     ma4\n      0.8288  -0.1250  -0.8547  0.1718  -0.0790  0.0694\ns.e.  0.3143   0.2688   0.3139  0.2685   0.0275  0.0205\n\nsigma^2 estimated as 0.0002787:  log likelihood = 6839.67,  aic = -13665.33\n\nTraining set error measures:\n                       ME       RMSE        MAE        MPE      MAPE      MASE\nTraining set 0.0002358921 0.01668977 0.01127632 0.00449418 0.2812394 0.9999891\n                    ACF1\nTraining set 6.16496e-05\n\n\n\n\n\n\n\n\n\n\n\nObservations:  The squared residuals plot shows a cluster of volatility, while the ACF and PACF still sees significant spikes at lags 1 - 36. Thus, the residuals show that there are still some patterns that need to be modeled by ARCH/GARCH models. These methods are useful in modeling the conditional variance of the series - I try ARCH(1) through ARCH(24).\n\n\nFit ARCH models and do model diagnostics\n\n\nOptimal ARCH order: 10\n\n::: {.cell-output .cell-output-stdout}\nMIN AIC: -14705.73\n\n:::\nIt looks like ARCH(10) on the residuals of ARIMA(2,1,4) produces the lowest AIC and is therefore the optimal ARIMA+ARCH model. It has an AIC value of -14705.73. The model equation is below, and I look to the Box Ljung test to ensure that the model adequately represents the residuals.\n\n\nModel Equation for ARIMA(2,1,4) + ARCH(10)\n\n\n\nCall:\narima(x = log.exxon, order = c(2, 1, 4))\n\nCoefficients:\n         ar1      ar2      ma1     ma2      ma3     ma4\n      0.8288  -0.1250  -0.8547  0.1718  -0.0790  0.0694\ns.e.  0.3143   0.2688   0.3139  0.2685   0.0275  0.0205\n\nsigma^2 estimated as 0.0002787:  log likelihood = 6839.67,  aic = -13665.33\n\nTraining set error measures:\n                       ME       RMSE        MAE        MPE      MAPE      MASE\nTraining set 0.0002358921 0.01668977 0.01127632 0.00449418 0.2812394 0.9999891\n                    ACF1\nTraining set 6.16496e-05\n\n\n\nCall:\ngarch(x = res.arima1, order = c(0, 10), trace = F)\n\nModel:\nGARCH(0,10)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-4.564144 -0.562552  0.002885  0.609414  5.021640 \n\nCoefficient(s):\n     Estimate  Std. Error  t value Pr(>|t|)    \na0  4.010e-05   3.367e-06   11.910  < 2e-16 ***\na1  1.441e-01   2.234e-02    6.451 1.11e-10 ***\na2  1.478e-01   2.513e-02    5.880 4.09e-09 ***\na3  1.307e-01   2.288e-02    5.711 1.12e-08 ***\na4  8.557e-02   2.040e-02    4.194 2.74e-05 ***\na5  8.641e-02   1.698e-02    5.090 3.58e-07 ***\na6  1.255e-02   1.502e-02    0.836  0.40336    \na7  5.356e-02   1.827e-02    2.933  0.00336 ** \na8  1.098e-01   2.065e-02    5.317 1.05e-07 ***\na9  1.595e-02   1.519e-02    1.050  0.29369    \na10 1.067e-01   2.270e-02    4.700 2.60e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nDiagnostic Tests:\n    Jarque Bera Test\n\ndata:  Residuals\nX-squared = 296.41, df = 2, p-value < 2.2e-16\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.06112, df = 1, p-value = 0.8047\n\n\n\n\nBox Ljung test results\n\n\n\n2) NEE Data\n\nACF/PACF plots of the returns squared to determine if there are any remaining dependence structures not captured by ARCH/GARCH models.\n\n\n\n\n\n\n\n\nThe ACF/PACF plot of returns(^2) help determine if ARCH is sufficient, or whether an AR/ARIMA model needs to be fit. From the ACF plot, I can observe that lags 0-3.0 are correlated with the time series. From the PACF plot I can observe that lags 0-3.0 are again correlated with the time series. Because there are still significant lags on both plots, an ARIMA model should be fitted first and an ARCH model can be fitted to the residuals of the ARIMA model.\n\n\nFit ARIMA model first to original data\n\na) Check if original data is stationary\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts.nee\nDickey-Fuller = -3.0825, Lag order = 13, p-value = 0.12\nalternative hypothesis: stationary\n\n\nExxon data is certainly not stationary, as seen from the ACF/PACF plots and ADF test shown above. Thus, to make the data stationary the log of Exxon Adjusted Closing Price is taken, and differencing is undertaken.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log.exxon %>% diff()\nDickey-Fuller = -13.168, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\nJust taking the log of EXXON stock data is not enough…the p-value of the ADF test is still 0.94, indicating that the data remains non-stationary. After differencing the data once, stationarity is achieved.\n\n\nb) Choose p,q,d values\nFrom the above ACF/PACF plots, values of p and q are chosen to investigate. Moving average order (found from ACF): q - 1,2,3,5 Autoregressive term (found from pACF): p - 1,2,3,5\n\n\nc) Model diagnostics\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n-13661.97\n-13650.27\n-13661.96\n\n\n0\n1\n1\n-13661.75\n-13644.20\n-13661.74\n\n\n0\n1\n2\n-13660.77\n-13637.39\n-13660.76\n\n\n0\n1\n3\n-13665.54\n-13636.30\n-13665.52\n\n\n0\n1\n4\n-13664.25\n-13629.17\n-13664.22\n\n\n0\n1\n5\n-13667.93\n-13627.01\n-13667.89\n\n\n1\n1\n0\n-13661.82\n-13644.28\n-13661.81\n\n\n1\n1\n1\n-13661.72\n-13638.33\n-13661.71\n\n\n1\n1\n2\n-13660.80\n-13631.57\n-13660.78\n\n\n1\n1\n3\n-13663.76\n-13628.68\n-13663.73\n\n\n1\n1\n4\n-13665.61\n-13624.68\n-13665.57\n\n\n1\n1\n5\n-13675.79\n-13629.02\n-13675.74\n\n\n2\n1\n0\n-13661.05\n-13637.66\n-13661.04\n\n\n2\n1\n1\n-13660.61\n-13631.37\n-13660.58\n\n\n2\n1\n2\n-13657.89\n-13622.81\n-13657.85\n\n\n2\n1\n3\n-13666.77\n-13625.85\n-13666.73\n\n\n2\n1\n4\n-13680.62\n-13633.85\n-13680.57\n\n\n2\n1\n5\n-13672.81\n-13620.19\n-13672.74\n\n\n3\n1\n0\n-13664.71\n-13635.48\n-13664.69\n\n\n3\n1\n1\n-13662.70\n-13627.62\n-13662.67\n\n\n3\n1\n2\n-13663.72\n-13622.79\n-13663.68\n\n\n3\n1\n3\n-13680.85\n-13634.07\n-13680.79\n\n\n3\n1\n4\n-13681.15\n-13628.53\n-13681.08\n\n\n4\n1\n0\n-13662.73\n-13627.65\n-13662.70\n\n\n4\n1\n1\n-13660.74\n-13619.81\n-13660.70\n\n\n4\n1\n2\n-13680.84\n-13634.06\n-13680.78\n\n\n4\n1\n3\n-13681.01\n-13628.38\n-13680.94\n\n\n5\n1\n0\n-13670.31\n-13629.38\n-13670.27\n\n\n5\n1\n1\n-13678.28\n-13631.51\n-13678.23\n\n\n5\n1\n2\n-13685.31\n-13632.69\n-13685.24\n\n\n\n\n\nMinimum AIC,BIC,and AICc\nThus, the ARIMA model that minimizes AIC and AICc is ARIMA(2,1,4).\nObservations:  The squared residuals plot shows a cluster of volatility, while the ACF and PACF still sees significant spikes at lags 0.0 - 1.5. Thus, the residuals show that there are still some patterns that need to be modeled by ARCH/GARCH models. These methods are useful in modeling the conditional variance of the series - I try ARCH(0.1) through ARCH(1.5).\n\n\n\nFit ARCH models and do model diagnostics\n\n\n[1] 3\n\n\nIt looks like ARCH(3) on the residuals of ARIMA(2,1,4) produces the lowest AIC and is therefore the optimal ARIMA+ARCH model.\n\n\nBox Ljung test results\n\n\nModel Equation\n\n\nCalculate residuals of ARIMA model\n\n\n\nCall:\narima(x = log.exxon, order = c(2, 1, 4))\n\nCoefficients:\n         ar1      ar2      ma1     ma2      ma3     ma4\n      0.8288  -0.1250  -0.8547  0.1718  -0.0790  0.0694\ns.e.  0.3143   0.2688   0.3139  0.2685   0.0275  0.0205\n\nsigma^2 estimated as 0.0002787:  log likelihood = 6839.67,  aic = -13665.33\n\nTraining set error measures:\n                       ME       RMSE        MAE        MPE      MAPE      MASE\nTraining set 0.0002358921 0.01668977 0.01127632 0.00449418 0.2812394 0.9999891\n                    ACF1\nTraining set 6.16496e-05"
  },
  {
    "objectID": "financial.html#exxon-data",
    "href": "financial.html#exxon-data",
    "title": "Financial Time Series Models",
    "section": "1) Exxon Data",
    "text": "1) Exxon Data\n\nACF/PACF plots of the returns squared\nUsed to determine if there are any remaining dependence structures not captured by ARCH/GARCH models.\n\n\n\n\n\n\n\n\nThe ACF/PACF plot of returns(^2) help determine if ARCH is sufficient, or whether an AR/ARIMA model needs to be fit. From the ACF plot, I can observe that lags 0-3.0 are correlated with the time series. From the PACF plot I can observe that lags 0-3.0 are again correlated with the time series. Because there are still significant lags on both plots, an ARIMA model should be fitted first and an ARCH model can be fitted to the residuals of the ARIMA model.\n\n\nFit ARIMA model to original data\n\na) Check if original data is stationary\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts.exxon\nDickey-Fuller = 0.055139, Lag order = 13, p-value = 0.99\nalternative hypothesis: stationary\n\n\nExxon data is certainly not stationary, as seen from the ACF/PACF plots and ADF test shown above. Thus, to make the data stationary the log of Exxon Adjusted Closing Price is taken, and differencing is undertaken.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log.exxon %>% diff()\nDickey-Fuller = -13.168, Lag order = 13, p-value = 0.01\nalternative hypothesis: stationary\n\n\nJust taking the log of EXXON stock data is not enough…the p-value of the ADF test is still 0.94, indicating that the data remains non-stationary. After differencing the data once, stationarity is achieved.\n\n\nb) Choose p,q,d values\nFrom the above ACF/PACF plots, values of p and q are chosen to investigate.  Moving average order (found from ACF): q - 1,2,3,5  Autoregressive term (found from pACF): p - 1,2,3,5\n\n\nc) Model diagnostics\nNote: There are 98 unique combinations of parameters so only the head of the resulting dataframe is shown.\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n1\n0\n-13661.97\n-13650.28\n-13661.96\n\n\n0\n1\n1\n-13661.75\n-13644.21\n-13661.74\n\n\n0\n1\n2\n-13660.78\n-13637.39\n-13660.76\n\n\n0\n1\n3\n-13665.54\n-13636.30\n-13665.52\n\n\n0\n1\n4\n-13664.25\n-13629.17\n-13664.22\n\n\n0\n1\n5\n-13667.94\n-13627.01\n-13667.89\n\n\n\n\n\nMinimum AIC,BIC,and AICc\nThus, the ARIMA model that minimizes AIC and AICc is ARIMA(2,1,4).\n\n\n\nCalculate residuals of ARIMA model\n\n\n\n\n\n\n\n\n\n\n\nObservations: The squared residuals plot shows a cluster of volatility, while the ACF and PACF still sees significant spikes at lags 1 - 36. Thus, the residuals show that there are still some patterns that need to be modeled by ARCH/GARCH models. These methods are useful in modeling the conditional variance of the series - I try ARCH(1) through ARCH(24).\n\n\nFit ARCH models and do model diagnostics\nI fit a series of GARCH models on the residuals of the ARIMA model, using parameters 0 - 10, and comparing the AIC between them.\n\n\nOptimal ARCH order: 10\n\n\nMIN AIC: -14705.73\n\n\nIt looks like ARCH(10) on the residuals of ARIMA(2,1,4) produces the lowest AIC and is therefore the optimal ARIMA+ARCH model. It has an AIC value of -14705.73. The model equation is below, and I look to the Box Ljung test to ensure that the model adequately represents the residuals. The Box Ljung test returns a p-value of 0.8046, so we can conclude that the model is a good fit.\n\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.061136, df = 1, p-value = 0.8047\n\n\n\n\nModel Equation for ARIMA(2,1,4) + ARCH(10)\n \\[\n\\phi(B)x_t = \\delta + \\theta(B)y_t\n\\]\nWhere\n\\[\n\\phi(B) = 1 - 0.8287(B) + 0.1250(B^2)\n\\]\n\\[\n\\theta(B) = 1 - 0.8547(B) - 0.1718(B^2) + 0.0790(B^3) - 0.0694(B^4)\n\\]\n\\[\nyt = \\sigma_t\\epsilon_t\n\\]\n\\[\n\\begin{aligned}\n  var(y_t|y_{t-1}) = 4.010e^{-5} + 1.441e^{-1}(y_{t-1})^2 + 1.478e^{-1}(y_{t-2})^2  \\\\\n  + 1.307e^{-1}(y_{t-3})^2 + 8.557e^{-2}(y_{t-4})^2 + 8.641e^{-2}(y_{t-5})^2 + \\\\\n  1.255e^{-2}(y_{t-6})^2 + 5.356e^{-2}(y_{t-7})^2 + 1.098e^{-1}(y_{t-8})^2 + \\\\\n  1.595e^{-2}(y_{t-9})^2 + 1.067e^{-1}(y_{t-10})^2\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "financial.html#nee-data",
    "href": "financial.html#nee-data",
    "title": "Financial Time Series Models",
    "section": "2) NEE Data",
    "text": "2) NEE Data\n\nACF/PACF plots of the returns squared\nUsed to determine if there are any remaining dependence structures not captured by ARCH/GARCH models.\n\n\n\n\n\n\n\n\nThe ACF/PACF plot of returns(^2) help determine if ARCH is sufficient, or whether an AR/ARIMA model needs to be fit. From the ACF plot, I can observe that lags 0-1.5 are correlated with the time series. From the PACF plot I can observe that lags 0-1.5 are again correlated with the time series. Because there are still significant lags on both plots, an ARIMA model should be fitted first and an ARCH model can be fitted to the residuals of the ARIMA model.\n\n\nFit ARIMA model first to original data\n\na) Check if original data is stationary\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  ts.nee\nDickey-Fuller = -3.0825, Lag order = 13, p-value = 0.12\nalternative hypothesis: stationary\n\n\nNEE data is not stationary, as seen from the ACF/PACF plots and ADF test shown above. Thus, to make the data stationary the log of Nee Adjusted Closing Price is taken, and differencing is undertaken. A drift term is also included in this analysis.\n\n\n\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  log.nee\nDickey-Fuller = -3.9308, Lag order = 13, p-value = 0.01244\nalternative hypothesis: stationary\n\n\nJust taking the log of NEE stock data results in a p-value of the ADF test of 0.01244. Thus, we could potentially difference again or not - this will be tested during model validation. The ACF/PACF plots of the logged and first differenced NEE stock data is shown above to assist with choosing p and q values.\n\n\nb) Choose p,q,d values\nFrom the above ACF/PACF plots, values of p and q are chosen to investigate.  Moving average order (found from ACF): q - 1,4,5,6  Autoregressive term (found from pACF): p - 1,4,5,6\n\n\nc) Model diagnostics\nNote: There are 98 unique combinations of parameters so only the head of the resulting dataframe is shown.\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n0\n0\n0\n-5708.761\n-5691.219\n-5708.751\n\n\n0\n1\n0\n-14598.639\n-14586.945\n-14598.634\n\n\n0\n0\n1\n-8686.282\n-8662.892\n-8686.266\n\n\n0\n1\n1\n-14604.572\n-14587.031\n-14604.563\n\n\n0\n0\n2\n-10461.055\n-10431.818\n-10461.031\n\n\n0\n1\n2\n-14605.293\n-14581.905\n-14605.278\n\n\n\n\n\nMinimum AIC and AICc\n\n\n   p d q       AIC       BIC     AICc\n48 3 0 3 -14675.87 -14623.24 -14675.8\n\n\n   p d q       AIC       BIC     AICc\n48 3 0 3 -14675.87 -14623.24 -14675.8\n\n\nThus, the ARIMA model that minimizes AIC and AICc is ARIMA(3,0,3).\n\n\n\nCalculate residuals of ARIMA model\n\n\n\n\n\n\n\n\n\n\n\nObservations: The squared residuals plot shows a cluster of volatility, while the ACF and PACF still sees significant spikes at lags 0 - 12. Thus, the residuals show that there are still some patterns that need to be modeled by ARCH/GARCH models. These methods are useful in modeling the conditional variance of the series - I try ARCH(1) through ARCH(12).\n\n\nFit ARCH models and do model diagnostics\n\n\nOptimal ARCH order: 8\n\n\nMIN AIC: -15308.39\n\n\nIt looks like ARCH(8) on the residuals of ARIMA(3,0,3) produces the lowest AIC and is therefore the optimal ARIMA+ARCH model. It has an AIC value of -15309.36. The model equation is below, and I look to the Box Ljung test to ensure that the model adequately represents the residuals. The Box Ljung test returns a p-value of 0.7907, so we can conclude that the model is a good fit.\n\n\n\n    Box-Ljung test\n\ndata:  Squared.Residuals\nX-squared = 0.06267, df = 1, p-value = 0.8023\n\n\n\n\nModel Equation for ARIMA(3,0,3) + ARCH(8)\n \\[\n\\phi(B)x_t = \\delta + \\theta(B)y_t\n\\]\nWhere\n\\[\n\\phi(B) = 1 + 0.6711(B) - 0.8547(B^2) - 0.8164(B^3)\n\\]\n\\[\n\\theta(B) = 1 + 1.6408(B) + 0.8040(B^2) + 0.0441(B^3)\n\\]\n\\[\nyt = \\sigma_t\\epsilon_t\n\\]\n\\[\n\\begin{aligned}\n  var(y_t|y_{t-1}) = 6.067e^{-5} + 1.107e^{-1}(y_{t-1})^2 + 8.325e^{-2}(y_{t-2})^2  \\\\\n  + 6.459e^{-2}(y_{t-3})^2 + 6.155e^{-2}(y_{t-4})^2 + 8.529e^{-2}(y_{t-5})^2 + \\\\\n  1.075e^{-1}(y_{t-6})^2 + 6.972e^{-2}(y_{t-7})^2 + 7.548e^{-1}(y_{t-8})^2\n\\end{aligned}\n\\]\n\nSource code for the above analysis: Github"
  },
  {
    "objectID": "deeplearning.html",
    "href": "deeplearning.html",
    "title": "Deep Learning for Time Series",
    "section": "",
    "text": "Using univariate time series, I here use deep learning to extract insights about energy in the United States. I use a Recurrent Neural Network (RNN), a Gated Recurrent Unit (GRU) network, and a Long-Short Term Memory Network (LSTM) to make future predictions related to CO2 Emissions. This analysis is in Python.\nNOTE: NN analysis on CO2 emissions data only due to sample size limitations. Because all the other datasets are on a yearly frequency we have a maximum of 50 datapoints per energy source - not enough to build a model with.\nPackages used: pandas, numpy, plotly, matplotlib, tensorflow, keras, sklearn"
  },
  {
    "objectID": "deeplearning.html#coal",
    "href": "deeplearning.html#coal",
    "title": "Deep Learning for Time Series",
    "section": "Coal",
    "text": "Coal\n\nSplit data into training and testing data\nAnd visualize split!\n\n\nTraining data size:  (519,)\nTest data size:  (130,)\n\n\n\n                                                \n\n\n\n\nReformat data into required shape\nRequired shape = (samples, time, features)\n\n\n\n                                                \n\n\n\n\nModel 1 = RNN\n\nModel architecture\n\n\nModel: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn (SimpleRNN)      (None, 3)                 15        \n                                                                 \n dense (Dense)               (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n\n\n2023-04-30 23:33:02.096265: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\n\nPlot training and validation loss (MSE)\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n(51, 9, 1) (51,) (51,) (12, 9, 1) (12,) (12,)\n(51,) (51,)\n0.03547626\n0.014106057\nTrain MSE = 0.03548 RMSE = 0.18835\nTest MSE = 0.01411 RMSE = 0.11877\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 18ms/step\n\n\n\n\n\n\n\n\nModel 2 - GRU\n\nModel Architecture\n\n\nModel: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru (GRU)                   (None, 3)                 54        \n                                                                 \n dense_1 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 58\nTrainable params: 58\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 6ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n(51, 9, 1) (51,) (51,) (5, 9, 1) (5,) (5,)\n(51,) (51,)\n0.03444274\n0.035602108\nTrain MSE = 0.03444 RMSE = 0.18559\nTest MSE = 0.03560 RMSE = 0.18869\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 24ms/step\n\n\n\n\n\n\n\n\nModel 3 - LSTM\n\nModel Architecture\n\n\nModel: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm (LSTM)                 (None, 3)                 60        \n                                                                 \n dense_2 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 64\nTrainable params: 64\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n(51, 9, 1) (51,) (51,) (5, 9, 1) (5,) (5,)\n(51,) (51,)\n0.034340642\n0.029136106\nTrain MSE = 0.03434 RMSE = 0.18531\nTest MSE = 0.02914 RMSE = 0.17069\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 18ms/step\n\n\n\n\n\n\n\n\nDiscussion\nFor predicting CO2 emissions from coal, the training RMSE are:  1) RNN - 0.18039 2) GRU - 0.18312 3) LSTM - 0.18691 \nAnd the test RMSE are: 1) RNN - 0.12964 2) GRU - 0.19448 3) LSTM - 0.14685\nThus, the RNN minimized RMSE for both training and validation predictions, while the GRU model overfit the data. Regularization is a technique used in ANNs to prevent overfitting and improve generalization performance. L2 regularization was used here, which adds a penalty term to the loss function that is proportional to the sum of the squares of the weights in the network. It was decently important here - without L2 regularization the test RMSE for RNN, GRU, and LSTM models were all over 0.2 which is much higher than their regularized RMSE above.\nForecasting here worked better than expected. For all three models I forecasted the next 5 years (60) months, and the forecasted plots appear to mimic the seasonality and spikes of the previous 30 years. Across all three models, the first 3-4 years are well forecasted and display the expected spikes in emissions during the summer. However, by year 5 forecast weakens and the nuance and variation in the trend is smaller.\nThe ARIMA(1,1,1)(0,1,2) forecast shared the same seasonality, but exhibited a downwards trend. The ANN models also exhibit a slight downward trend, but are far more consistent over time. The RNSE from the ARIMA model was 0.0433, significantly smaller than the ANN RMSE. With more model tuning the ANN could likely outperform the ARIMA model."
  },
  {
    "objectID": "deeplearning.html#hydro",
    "href": "deeplearning.html#hydro",
    "title": "Deep Learning for Time Series",
    "section": "Hydro",
    "text": "Hydro\n\nSplit data into training and testing data\nAnd visualize split!\n\n\nTraining data size:  (519,)\nTest data size:  (130,)\n\n\n\n                                                \n\n\n\n\nReformat data into required shape\nRequired shape = (samples, time, features)\n\n\n\n                                                \n\n\n\n\nModel 1 = RNN\n\nModel architecture\n\n\nModel: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_1 (SimpleRNN)    (None, 3)                 15        \n                                                                 \n dense_3 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss (MSE)\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 3ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 21ms/step\n(51, 9, 1) (51,) (51,) (12, 9, 1) (12,) (12,)\n(51,) (51,)\n0.0420848\n0.009427733\nTrain MSE = 0.04208 RMSE = 0.20515\nTest MSE = 0.00943 RMSE = 0.09710\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 22ms/step\n\n\n\n\n\n\n\n\nModel 2 - GRU\n\nModel Architecture\n\n\nModel: \"sequential_4\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_1 (GRU)                 (None, 3)                 54        \n                                                                 \n dense_4 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 58\nTrainable params: 58\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 3ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n(51, 9, 1) (51,) (51,) (5, 9, 1) (5,) (5,)\n(51,) (51,)\n0.039451856\n0.0008718489\nTrain MSE = 0.03945 RMSE = 0.19862\nTest MSE = 0.00087 RMSE = 0.02953\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n\n\n\nModel 3 - LSTM\n\nModel Architecture\n\n\nModel: \"sequential_5\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_1 (LSTM)               (None, 3)                 60        \n                                                                 \n dense_5 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 64\nTrainable params: 64\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n(51, 9, 1) (51,) (51,) (5, 9, 1) (5,) (5,)\n(51,) (51,)\n0.03453453\n0.0061190366\nTrain MSE = 0.03453 RMSE = 0.18583\nTest MSE = 0.00612 RMSE = 0.07822\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n\n\n\nDiscussion\nFor predicting CO2 emissions from Hydro Power, the training RMSE are:  1) RNN - 0.17576 2) GRU - 0.19256 3) LSTM - 0.18912 \nAnd the test RMSE are: 1) RNN - 0.07061 2) GRU - 0.02567 3) LSTM - 0.07663\nThus, the RNN minimized RMSE for training predictions, but the GRU ANN produced the lowest test RMSE of the group. Test RMSE is significantly lower than training RMSE, so further analysis could investigate possible data leakage. Regularization again improved the model performance, by 0.04 points in the RMSE.\nForecasting here worked better than expected. For all three models I forecasted the next 5 years (60) months, and the forecasted plots appear to mimic the seasonality and spikes of the previous 30 years. Across all three models, the first 4 years are well forecasted and display the expected spikes in emissions during the summer. However, by year 5 forecast weakens and the nuance and variation in the trend is smaller.\nThe ARIMA(1,1,2)(0,1,1) forecast shared the same seasonality, but exhibited an upwards trend across all 5 years. The ANN models exhibit no trend (only seasonality). The RNSE from the ARIMA model was 0.07292737, significantly smaller than the ANN RMSE. With more model tuning the ANN could likely outperform the ARIMA model."
  },
  {
    "objectID": "deeplearning.html#wind",
    "href": "deeplearning.html#wind",
    "title": "Deep Learning for Time Series",
    "section": "Wind",
    "text": "Wind"
  },
  {
    "objectID": "deeplearning.html#solar",
    "href": "deeplearning.html#solar",
    "title": "Deep Learning for Time Series",
    "section": "Solar",
    "text": "Solar"
  },
  {
    "objectID": "deeplearning.html#total",
    "href": "deeplearning.html#total",
    "title": "Deep Learning for Time Series",
    "section": "Total",
    "text": "Total\nSource code for the above analysis: [Github](https://github.com/eliserust/TimeSeries/blob/main/nn.qmd)"
  },
  {
    "objectID": "deeplearning.html#natural-gas",
    "href": "deeplearning.html#natural-gas",
    "title": "Deep Learning for Time Series",
    "section": "Natural Gas",
    "text": "Natural Gas\n\nSplit data into training and testing data\nAnd visualize split!\n\n\nTraining data size:  (519,)\nTest data size:  (130,)\n\n\n\n                                                \n\n\n\n\nReformat data into required shape\nRequired shape = (samples, time, features)\n\n\n\n                                                \n\n\n\n\nModel 1 = RNN\n\nModel architecture\n\n\nModel: \"sequential_6\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_2 (SimpleRNN)    (None, 3)                 15        \n                                                                 \n dense_6 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss (MSE)\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n(51, 9, 1) (51,) (51,) (12, 9, 1) (12,) (12,)\n(51,) (51,)\n0.041223448\n0.0054442543\nTrain MSE = 0.04122 RMSE = 0.20304\nTest MSE = 0.00544 RMSE = 0.07379\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 33ms/step\n\n\n\n\n\n\n\n\nModel 2 - GRU\n\nModel Architecture\n\n\nModel: \"sequential_7\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_2 (GRU)                 (None, 3)                 54        \n                                                                 \n dense_7 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 58\nTrainable params: 58\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 3ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n(51, 9, 1) (51,) (51,) (5, 9, 1) (5,) (5,)\n(51,) (51,)\n0.036575884\n0.006833242\nTrain MSE = 0.03658 RMSE = 0.19125\nTest MSE = 0.00683 RMSE = 0.08266\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n\n\n\nModel 3 - LSTM\n\nModel Architecture\n\n\nModel: \"sequential_8\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_2 (LSTM)               (None, 3)                 60        \n                                                                 \n dense_8 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 64\nTrainable params: 64\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 19ms/step\n(51, 9, 1) (51,) (51,) (5, 9, 1) (5,) (5,)\n(51,) (51,)\n0.03401606\n0.044011094\nTrain MSE = 0.03402 RMSE = 0.18443\nTest MSE = 0.04401 RMSE = 0.20979\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 20ms/step\n\n\n\n\n\n\n\n\nDiscussion\nFor predicting CO2 emissions from Natural Gas, the training RMSE are:  1) RNN - 0.19666 2) GRU - 0.19114 3) LSTM - 0.18312 \nAnd the test RMSE are: 1) RNN - 0.05314 2) GRU - 0.07994 3) LSTM - 0.21063\nThus, the GRU model minimized RMSE for training predictions, while the RNN minimized RMSE for test predictions. Similarly to Hydro modeling, there is a pretty big difference between train and test RMSE for RNN and GRU models here. This could indicate data leakage and should be investigated via further analysis. The LSTM, on the other hand, appears to be overfitted as the test RMSE is higher than the train RMSE. Regularization again improved the model performance slightly, by 0.02 points in the RMSE.\nForecasting here worked better than expected. For all three models I forecasted the next 5 years (60) months, and the forecasted plots appear to mimic the seasonality and spikes of the previous 30 years. Across all three models, the first 4 years are well forecasted and display the expected spikes in emissions during the summer. However, by year 5 forecast weakens and the nuance and variation in the trend is smaller.\nThe ARIMA(1,1,1)(0,1,2) forecast shared the same seasonality, but exhibited an upwards trend across all 5 years. The ANN models exhibit no trend (only seasonality). The RNSE from the ARIMA model was 0.04678813, only slightly smaller than the ANN RMSE. The RNN had an RMSE of 0.05314, so with more model tuning could surely outperform the ARIMA model."
  },
  {
    "objectID": "deeplearning.html#petroleum",
    "href": "deeplearning.html#petroleum",
    "title": "Deep Learning for Time Series",
    "section": "Petroleum",
    "text": "Petroleum\n\nSplit data into training and testing data\nAnd visualize split!\n\n\nTraining data size:  (519,)\nTest data size:  (130,)\n\n\n\n                                                \n\n\n\n\nReformat data into required shape\nRequired shape = (samples, time, features)\n\n\n\n                                                \n\n\n\n\nModel 1 = RNN\n\nModel architecture\n\n\nModel: \"sequential_9\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n simple_rnn_3 (SimpleRNN)    (None, 3)                 15        \n                                                                 \n dense_9 (Dense)             (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 19\nTrainable params: 19\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss (MSE)\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 3ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 22ms/step\n(51, 9, 1) (51,) (51,) (12, 9, 1) (12,) (12,)\n(51,) (51,)\n0.035382252\n0.008783025\nTrain MSE = 0.03538 RMSE = 0.18810\nTest MSE = 0.00878 RMSE = 0.09372\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 22ms/step\n\n\n\n\n\n\n\n\nModel 2 - GRU\n\nModel Architecture\n\n\nModel: \"sequential_10\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n gru_3 (GRU)                 (None, 3)                 54        \n                                                                 \n dense_10 (Dense)            (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 58\nTrainable params: 58\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n(51, 9, 1) (51,) (51,) (5, 9, 1) (5,) (5,)\n(51,) (51,)\n0.035191447\n0.017926276\nTrain MSE = 0.03519 RMSE = 0.18759\nTest MSE = 0.01793 RMSE = 0.13389\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n\n\n\nModel 3 - LSTM\n\nModel Architecture\n\n\nModel: \"sequential_11\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n lstm_3 (LSTM)               (None, 3)                 60        \n                                                                 \n dense_11 (Dense)            (None, 1)                 4         \n                                                                 \n=================================================================\nTotal params: 64\nTrainable params: 64\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\n\nPlot training and validation loss\n\n\n1/2 [==============&gt;...............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b2/2 [==============================] - 0s 2ms/step\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 16ms/step\n(51, 9, 1) (51,) (51,) (5, 9, 1) (5,) (5,)\n(51,) (51,)\n0.034488864\n0.016665712\nTrain MSE = 0.03449 RMSE = 0.18571\nTest MSE = 0.01667 RMSE = 0.12910\n\n\n\n                                                \n\n\n\n\nForecasting - next 5 years (60 months)\n\n\n1/1 [==============================] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 [==============================] - 0s 17ms/step\n\n\n\n\n\n\n\n\nDiscussion\nFor predicting CO2 emissions from petroleum, the training RMSE are:  1) RNN - 0.18713 2) GRU - 0.18709 3) LSTM - 0.18830 \nAnd the test RMSE are: 1) RNN - 0.09508 2) GRU - 0.14254 3) LSTM - 0.13025\nThus, the GRU model minimized RMSE for training predictions, while the RNN model minimized RMSE for test predictions. However, the RNN model may suffer from data leakage so the GRU model is likely the best model for predicting natural gas CO2 emissions going forward. Regularization improved the model performance.\nForecasting here worked better than expected. For all three models I forecasted the next 5 years (60) months, and the forecasted plots appear to mimic the seasonality and spikes of the previous 30 years. Across all three models, the first 4 years are well forecasted and display the expected spikes in emissions during the summer. However, by year 5 forecast weakens and the nuance and variation in the trend is smaller.\nThe SARIMA(0,1,1)(1,1,1) forecast shared the same seasonality, but exhibited a slight downwards trend across all 5 years. The ANN models exhibit no trend (only seasonality). The RMSE from the ARIMA model was 0.04678813, only slightly smaller than the ANN RMSE. The RNN had an RMSE of 0.09508, so with more model tuning could likely outperform the ARIMA model."
  }
]